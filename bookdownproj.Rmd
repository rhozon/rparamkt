--- 
title: "R para Estudantes de Marketing"
author: "KU Leuven Marketing department (tradução: Rodrigo Hermont Ozon)"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---

*** 

# Sobre o autor desta tradução {.unlisted .unnumbered}

![](https://github.com/rhozon/site/raw/master/me.jpg)

_Rodrigo Hermont Ozon,_ economista e apaixonado por econometria, pelas aplicações de modelos econômicos a problemas reais e cotidianos vivenciados na sociedade e na realidade do mundo empresarial e corporativo.

Seus contatos podem ser acessados em:


- [WebSite](https://rhozon.github.io/) 

- [LinkeDin](https://www.linkedin.com/in/rodrigohermontozon/) 



### Resumo {.unlisted .unnumbered}

  O objetivo de traduzir esse tutorial consiste em facilitar o aprendizado e utilização da linguagem estatística R para os profissionais de marketing e demais áreas de negócio que precisam se adequar a uma realidade mutante e movida por um fluxo significativo de informações por todos os lados. Trabalhar e interpretar bem os dados é um desafio computacional para muitos profissionais dessa área; e esta tradução visa cobrir (ainda que superficialmente) tal lacuna.
  
  Este bookdown foi escrito no inicialmente no [overleaf](http://www.overleaf.com) com o pacote knitr [para a página interativa de autoria de KU Leven Marketing Department.](https://bookdown.org/content/1340/)
  
***


Ao meu amado paizão e professor pra vida inteira, Ronaldo --  _"Ensina a criança no caminho em que deve andar, e, ainda quando for velho, não se desviará dele."_




\hspace{13cm}[Provérbios 22:6](https://bibliaportugues.com/proverbs/22-6.htm)


#### Nota do tradutor {.unlisted .unnumbered}

Esse e-book traduzido é oriundo de [_R for marketing students_](https://bookdown.org/content/1340/).

\vspace{5cm}



_As traduções aqui são somente as transcrições. Não me preocupei em aperfeiçoá-las para a língua portuguesa com maior nível de clareza nos textos. As figuras e imagens não foram traduzidas._

***

# Sobre este tutorial {.unlisted .unnumbered}


Neste tutorial, exploraremos o R como uma ferramenta para analisar e visualizar dados. O R é uma linguagem de programação estatística que rapidamente ganhou popularidade em muitos campos científicos. A principal diferença entre o R e outro _software_ estatístico como o SPSS é que o R não possui interface gráfica com o usuário. Não há botões para clicar. O R é executado inteiramente digitando comandos em uma interface de texto. Isso pode parecer assustador, mas, esperançosamente, no final deste tutorial, você verá como o R pode ajudá-lo a fazer uma melhor análise estatística.

Então, por que estamos usando R e não um dos muitos outros pacotes estatísticos como SPSS, SAS ou Microsoft Excel? Algumas das razões mais importantes:

Ao contrário de outros _softwares_, o R é gratuito e de código aberto, e sempre será!
R é uma linguagem de programação e não uma interface gráfica como o SPSS. Ele realiza análises ou visualizações executando algumas linhas de código. Essas linhas de código podem ser salvas como scripts para repetição futura das análises ou visualizações. Também facilita o compartilhamento de seu trabalho com outras pessoas, que podem aprender ou corrigi-lo se houver algum erro.

O R tem uma comunidade online muito ativa e útil. Quando você se depara com um problema, muitas vezes basta uma rápida pesquisa no Google para encontrar uma solução de origem coletiva.


Todas as principais empresas de pesquisa de marketing indicam que estão experimentando o R e que o R é o software do futuro.
Este tutorial se concentra em análises estatísticas relevantes para estudantes de marketing. Se você quiser uma introdução mais extensa, porém acessível, ao R, confira o excelente e gratuito livro "[R for Data Science](http://r4ds.had.co.nz/index.html)". Este capítulo introdutório e o próximo são baseados na introdução ao R, encontrada nos tutoriais do [_Coding Club_](https://ourcodingclub.github.io/), que também possui muitos outros ótimos tutoriais de R.

Este tutorial foi escrito no RMarkdown, com a ajuda do incrível [pacote bookdown.](https://bookdown.org/yihui/bookdown/)

Questões? Comentários? Sugestões? Envie-me um e-mail: [samuel.franssens@kuleuven.be](mailto:samuel.franssens@kuleuven.be)

## Download e instalação do R e do RStudio {.unlisted .unnumbered}

Para fornecer algumas funcionalidades extras e facilitar um pouco a transição, usaremos um programa chamado R Studio como um front-end gráfico para o R.

Você pode fazer o download do R em [https://cloud.r-project.org/](https://cloud.r-project.org/). Selecione o link apropriado para o seu sistema operacional e instale o R no seu computador (no Windows, você primeiro precisa clicar em "base").

Em seguida, faça o download do R Studio em [https://www.rstudio.com/products/rstudio/download/](https://www.rstudio.com/products/rstudio/download/). Selecione o instalador para a versão gratuita e instale o R Studio (nota: você precisa ter o R instalado primeiro).

## Obtendo familiaridade com o \faRProject Studio {.unlisted .unnumbered}



### Console vs. script {.unlisted .unnumbered}


![](https://bookdown.org/content/1340/images/rstudio.png)

Ao abrir o R Studio, você verá uma janela como a acima. Você pode digitar o código diretamente no console (janela inferior esquerda) - basta digitar seu código após o prompt ($>$) e pressionar enter no final da linha para executar o código. Você também pode escrever seu código no arquivo de script (a janela superior esquerda). Se você não ver uma janela com um arquivo de script, abra uma clicando em Arquivo, Novo arquivo, R Script. Para executar uma linha de código a partir do seu script, pressione Ctrl + R ou Ctrl + Enter no Windows e Cmd + Enter no Mac ou use o botão 'Executar' no canto superior direito da janela do script.

O código digitado diretamente no console não será salvo pelo R. O código digitado em um arquivo de script pode ser salvo como um registro reproduzível de sua análise. Se você estiver trabalhando no console e quiser editar ou executar novamente uma linha de código anterior, pressione a seta para cima. Se você estiver trabalhando em um script, lembre-se de clicar em Salvar frequentemente (Arquivo, Salvar), para que você realmente salve o seu script!

É melhor trabalhar em arquivos de script. Também é altamente recomendável salvar seu arquivo de script em uma pasta que é automaticamente copiada pelo software de compartilhamento de arquivos que oferece a funcionalidade "versões anteriores" (o [Dropbox](https://www.dropbox.com/business/landing-t61fl-v2?_tk=paid_sem_goog_biz_b&_camp=1411326950&_kw=dropbox|e&_ad=389658655593||c&gclid=CjwKCAjwtqj2BRBYEiwAqfzur1JwrYwkXrKtxYHo_nfrnCMIUBe-IpmrIgCZmTt0l3gibHmKgokdvBoChx8QAvD_BwE) é provavelmente o mais famoso; aqui estão algumas [alternativas](https://medium.com/@Vanina/dropbox-alternatives-top-5best-cloud-storage-services-2017-a703af7d7796). Isso lhe dará a opção de restaurar versões salvas anteriormente de seus arquivos sempre que você salvar algo por engano. Como qualquer peça escrita, os roteiros se beneficiam de estrutura e clareza - a [Coding Etiquette do Coding Club](https://ourcodingclub.github.io/2017/04/25/etiquette.html) oferece mais conselhos sobre isso.


## Comentários {.unlisted .unnumbered}

Ao escrever um script, é muito importante adicionar comentários para descrever o que você está fazendo e por quê. Você pode fazer isso inserindo um \# na frente de uma linha de texto. Comece seu script gravando quem está escrevendo o roteiro, a data e o objetivo principal - no capítulo introdutório, aprenderemos sobre as acomodações do Airbnb na Bélgica. Aqui está um exemplo:

```{r}
# Aprendendo a importar e explorar dados e criar graficos investigando as acomodacoes do Airbnb na Belgica
# Escrito por Samuel Franssens 28/01/2018
```


## Pacotes {.unlisted .unnumbered}

As próximas linhas de código geralmente carregam os pacotes que você usará em sua análise ou visualização. 

O R carrega automaticamente várias funções para executar operações básicas, mas os pacotes fornecem funcionalidade extra. Eles geralmente consistem em várias funções que podem lidar com tarefas específicas. Por exemplo, um pacote poderia fornecer funções para fazer análises de cluster ou para fazer biplots. Para instalar um pacote, digite install.packages ("nome do pacote") (e pressione enter ao trabalhar no console ou pressione Ctrl + Enter, Ctrl + R, Cmd + Enter ou o botão 'Executar' ao trabalhar em um script).

Você só precisa instalar pacotes uma vez; depois, basta carregá-los usando a biblioteca (nome do pacote). Aqui, usaremos o popular pacote tidyverse que fornece muitas funções úteis e intuitivas

 ( [https://www.tidyverse.org/](https://www.tidyverse.org/) ).
 
O pacote tidyverse é na verdade uma coleção de outros pacotes; portanto, durante a instalação ou o carregamento, você verá que vários pacotes são instalados ou carregados. Instale e carregue o pacote tidyverse executando as seguintes linhas de código:

```{r}
#install.packages(tidyverse) # instala o pacote tidyverse
library(tidyverse) # carrega o pacote tidyverse

```

Observe que há aspas ao instalar um pacote, mas não ao carregá-lo.

A instalação de um pacote normalmente produz muita saída no console. Você pode verificar se instalou um pacote com êxito, carregando o pacote. Se você tentar carregar um pacote que não foi instalado com sucesso, você receberá o seguinte erro:

```{r}
#library(marketing) # Estou tentando instalar o pacote inexistente 'marketing'
## Error in library(marketing): there is no package called 'marketing'
```

Nesse caso, tente reinstalar o pacote.

Quando você tenta usar uma função de um determinado pacote que ainda não foi carregado, você pode receber o seguinte erro:

```{r}
# agnes eh uma funcao do pacote cluster para rodar analise de cluster.
#agnes(dist(data), metric = "euclidean", method = "ward")

## Error in agnes(dist(data), metric = "euclidean", method = "ward"): could not find function "agnes"
```


O R nos dirá que não pode encontrar a função solicitada (neste caso, agnes, uma função do pacote de cluster para análises de cluster). Geralmente, isso ocorre porque você ainda não carregou (ou instalou) o pacote ao qual a função pertence.

Após instalar e carregar o pacote tidyverse, você poderá usar as funções incluídas no pacote tidyverse. Como você usará o pacote tidyverse com tanta frequência, é melhor sempre carregá-lo no início do seu script.



<!--chapter:end:index.Rmd-->

# Introdução ao R {.unlisted .unnumbered}

Neste capítulo introdutório, você aprenderá:


- como importar dados
- como manipular um conjunto de dados com o operador de canal
- como resumir um conjunto de dados
- como fazer gráficos de dispersão e histogramas

## Importando dados {.unlisted .unnumbered}

Neste capítulo, exploraremos um conjunto de dados publicamente disponível dos dados do Airbnb. Encontramos esses dados [aqui.](http://tomslee.net/airbnb-data-collection-get-the-data) (Estes são dados reais “raspados” do airbnb.com em julho de 2017. Isso significa que o proprietário do site criou um script para coletar automaticamente esses dados no site airbnb.com. Essa é uma das muitas coisas que você também pode fazer no R. Mas primeiro vamos aprender o básico.) Você pode baixar o conjunto de dados clicando com o botão direito do mouse [neste link](http://users.telenet.be/samuelfranssens/tutorial_data/tomslee_airbnb_belgium_1454_2017-07-14.csv), selecionando “Salvar link como…” (ou algo semelhante) e salvando o arquivo .csv em um diretório no disco rígido. Como mencionado na [introdução ](https://bookdown.org/content/1340/getting-familiar-with-rstudio.html#console_script), é uma boa ideia salvar seu trabalho em um diretório que é automaticamente copiado pelo software de compartilhamento de arquivos. Mais tarde, salvaremos nosso script no mesmo diretório.

## Importando arquivos .csv {.unlisted .unnumbered}

![](https://bookdown.org/content/1340/images/rstudio2.png)

Para importar dados para o R, clique em Import Dataset e depois em From text (readr). Uma nova janela será exibida. Clique em Procurar e encontre seu arquivo de dados. Certifique-se de que Primeira linha como nomes esteja selecionada (isso diz ao R para tratar a primeira linha dos seus dados como os títulos das colunas) e clique em Importar. Após clicar em importar, o R Studio abre uma guia Visualizador. Isso mostra seus dados em uma planilha.

Alguns computadores salvam arquivos .csv com ponto e vírgula (;) em vez de vírgulas (,) como separadores ou "delimitadores". Isso geralmente acontece quando o inglês não é o primeiro ou o único idioma do seu computador. Se seus arquivos estiverem separados por ponto e vírgula, clique em Importar conjunto de dados e encontre seu arquivo de dados, mas agora escolha Ponto e vírgula no delimitador do menu suspenso.

Nota: se você não salvou o conjunto de dados clicando com o botão direito do mouse no link e selecionando “Salvar link como…”, mas clicou com o botão esquerdo do mouse no link, seu navegador pode ter acabado abrindo o conjunto de dados. Você pode salvar o conjunto de dados pressionando Ctrl + S. Observe, no entanto, que seu navegador pode acabar salvando o conjunto de dados como um arquivo .txt. É importante alterar a extensão do seu arquivo nos argumentos para o comando read\_csv abaixo.


## Ajustando seu diretório de trabalho  {.unlisted .unnumbered}

Depois de importar seus dados com Import Dataset, verifique a janela do console. Você verá o comando para abrir o Visualizador (View()) e, uma linha acima, verá o comando que lê os dados. Copie o comando que lê os dados do console para o seu script. No meu caso, fica assim:

```{r}
#tomslee_airbnb_belgium_1454_2017_07_14 <- read.csv("c:/Dropbox/work/teaching/R/data/tomslee_airbnb_belgium_1454_2017-07-14.csv") 
# Mude .csv para .txt se necessario

airbnb<-read.csv(file="http://users.telenet.be/samuelfranssens/tutorial_data/tomslee_airbnb_belgium_1454_2017-07-14.csv", sep=",", head=TRUE)
```

Esta linha tem a seguinte leitura (da direita para a esquerda): a funcao read\_csv deve ler o arquivo tomslee\_airbnb\_belgium\_1454\_2017-07-14.csv no diretorio c: / Dropbox / work / teaching / R / data / (voce vera um diretorio diferente aqui ) Em seguida, R deve atribuir ($<-$) esses dados a um objeto chamado tomslee\_airbnb\_belgium\_1454\_2017\_07\_14.

Antes de explicar cada um desses conceitos, vamos simplificar esta linha de código:

```{r}
#setwd("c:/Dropbox/work/teaching/R/data/") # Ajusta o diretorio de trabalho para onde o R precisa apontar para o arquivo .csv
#airbnb <- read_csv("tomslee_airbnb_belgium_1454_2017-07-14.csv") 
# read_csv agora nao precisa mais de um diretorio e somente precisa de um nome de arquivo
# Atribuimos os dados a um objeto com um nome mais simples: airbnb em vez de tomslee_airbnb_belgium_1454_2017_07_14
```

O comando setwd informa ao R onde está o seu diretório de trabalho. Seu diretório de trabalho é uma pasta no seu computador onde o R procurará dados, onde as plotagens serão salvas etc. Defina seu diretório de trabalho na pasta em que os dados foram armazenados. Agora, o arquivo read\_csv não requer mais um diretório.

Você só precisa definir seu diretório de trabalho uma vez, na parte superior do seu script. Você pode verificar se está definido corretamente executando getwd(). Observe que em um computador com Windows, os caminhos de arquivo possuem barras invertidas que separam as pastas ("C: \ folder \ data"). 

No entanto, o caminho do arquivo digitado no R deve usar barras ("C: / folder / data").

Salve este script no diretório de trabalho (no meu caso: c: / Dropbox / trabalho / ensino / R / dados /)). No futuro, você pode simplesmente executar essas linhas de código para importar seus dados em vez de clicar em Importar conjunto de dados (a execução de linhas de código é muito mais rápida do que apontar e clicar - uma das vantagens do uso do R).

Não se esqueça de carregar o pacote tidyverse na parte superior do seu script (mesmo antes de definir o diretório de trabalho) com a biblioteca (tidyverse).


## Atribuindo dados a objetos {.unlisted .unnumbered}

Observe a seta $<-$ no meio da linha que importou o arquivo .csv:

```{r}
#airbnb <- read_csv("tomslee_airbnb_belgium_1454_2017-07-14.csv")
```

$<-$ é o operador de atribuição. Nesse caso, atribuímos o conjunto de dados (ou seja, os dados que lemos do arquivo .csv) a um objeto chamado airbnb. Um objeto é uma estrutura de dados. Todos os objetos que você criar serão exibidos no painel Ambiente (a janela superior direita). O R Studio fornece um atalho para escrever $<-$: Alt + - (no Windows). É uma boa ideia aprender esse atalho de cor.

Quando você importa dados para o R, ele se torna um objeto chamado quadro de dados. Um quadro de dados é como uma tabela ou uma planilha do Excel. Tem duas dimensões: linhas e colunas. Geralmente, as linhas representam suas observações, as colunas representam as diferentes variáveis. Quando seus dados consistem em apenas uma dimensão (por exemplo, uma sequência de números ou palavras), eles são armazenados em um segundo tipo de objeto chamado vetor. Mais tarde, aprenderemos como criar vetores.

## Importando arquivos do Excel {.unlisted .unnumbered}

O R funciona melhor com arquivos .csv (valores separados por vírgula). No entanto, os dados geralmente são armazenados como um arquivo do Excel (você pode baixar o conjunto de dados do Airbnb como um arquivo do Excel aqui). O R também pode lidar com isso, mas você precisará carregar primeiro um pacote chamado readxl (este pacote faz parte do pacote tidyverse, mas não é carregado com a biblioteca (tidyverse) porque não é um pacote tidyverse principal):

```{r}
library(readxl) # carrega o pacote

#airbnb.excel <- read_excel (path = "tomslee_airbnb_belgium_1454_2017-07-14.xlsx", sheet = "Sheet1")
# verifique se o arquivo do Excel está salvo no seu diretório de trabalho

# você também pode deixar de fora o path = & sheet =
# então o comando se torna: read_excel ("tomslee_airbnb_belgium_1454_2017-07-14.xlsx", "Sheet1")
```

read\_excel é uma função do pacote readxl. São necessários dois argumentos: o primeiro é o nome do arquivo e o segundo é o nome da planilha do Excel que você deseja ler.

## Lendo os dados do Airbnb {.unlisted .unnumbered}

Nosso conjunto de dados contém informações sobre quartos na Bélgica listados no airbnb.com. Sabemos para cada sala (identificada por room\_id): quem é o hóspede (host\_id), que tipo de sala é (room\_type), onde está localizada (country, city, neighborhood e até a latitude e longitude exata), como muitas críticas que recebeu (reviews), como as pessoas estavam satisfeitas (overall\_satisfaction), preço (price) e características dos quartos (accommodates, bedrooms, bathrooms, minstay).

Uma etapa realmente importante é verificar se seus dados foram importados corretamente. É uma boa prática sempre inspecionar seus dados. Você vê algum valor ausente, os números e os nomes fazem sentido? Se você começar imediatamente com a análise, corre o risco de ter que refazê-la porque os dados não foram lidos corretamente, ou pior, analisando dados errados sem perceber.


```{r}
airbnb[1:10,] # Visualiza o conteudo do conjunto de dados da Airbnb para as dez primeiras linhas

```

O R nos diz que estamos lidando com uma __tibble_ (essa é apenas outra palavra para quadro de dados) com 17651 linhas ou observações e 20 colunas ou variáveis. Para cada coluna, é fornecido o tipo da variável: int (inteiro), chr (caractere), dbl (duplo), dttm (data e hora). Variáveis inteiras e duplas armazenam números (inteiro para números redondos, duplicam para números com decimais), variáveis de caracteres armazenam letras, variáveis de data e hora armazenam datas e / ou horas.

O R imprime apenas os dados das dez primeiras linhas e o número máximo de colunas que cabem na tela. Se, no entanto, você deseja inspecionar todo o conjunto de dados, clique duas vezes no objeto airbnb no painel Ambiente (a janela superior direita) para abrir uma aba Visualizador ou executar a Visualização (airbnb). Observe o V maiúsculo no comando Visualizar. O R sempre diferencia maiúsculas de minúsculas!

Você também pode usar o comando print para solicitar mais (ou menos) linhas e colunas na janela do console:

```{r}
# Imprima 25 linhas (defina como Inf para imprimir todas as linhas) e defina a largura como 100 para ver mais colunas.
# Observe que as colunas que nao cabem na primeira tela com 25 linhas
# sao impressos abaixo das 25 linhas iniciais.

#print (airbnb, n = 25, width = 100)

airbnb[1:25,]
```



## Manipulando dataframes {.unlisted .unnumbered}
### Transformando variáveis {.unlisted .unnumbered}
#### Fatoração {.unlisted .unnumbered}

Vamos observar nosso dataset novamente:

```{r}
airbnb[1:20,1:8]
```

Vimos que room\_id e host\_id são "identificadores" ou rótulos que identificam as observações. São nomes (neste caso, apenas números) para as salas(quartos) e hóspedes específicos. No entanto, vemos que o R os trata como números inteiros, ou seja, como números. Isso significa que poderíamos adicionar os room\_id‘s de duas salas diferentes e obter um novo número. No entanto, isso não faria muito sentido, porque os room\_id são apenas rótulos.

Certifique-se de que R trate os identificadores como rótulos, em vez de números, fatorando-os. Observe o operador \$. Este operador muito importante nos permite selecionar variáveis específicas de um quadro de dados, neste caso room\_id e host\_id.

```{r}
airbnb$room_id_F <- factor(airbnb$room_id)
airbnb$host_id_F <- factor(airbnb$host_id)
```

Uma variável de fator é semelhante a uma variável de caractere, pois armazena letras. Os fatores são mais úteis para variáveis que podem assumir apenas um número de categorias pré-determinadas. Eles devem, por exemplo, ser usados para variáveis dependentes categóricas - por exemplo, se uma venda foi feita ou não: venda _versus_ não venda. Você pode pensar em fatores como variáveis que armazenam rótulos. Os rótulos reais não são tão importantes (não nos importamos se uma venda é chamada de venda ou sucesso ou algo mais), apenas os usamos para fazer uma distinção entre categorias diferentes. É muito importante fatorar variáveis inteiras que representam variáveis independentes ou dependentes categóricas, porque, se não fatorarmos essas variáveis, elas serão tratadas como contínuas em vez de variáveis categóricas nas análises. Por exemplo, uma variável pode representar uma venda como 1 e uma não-venda como 0. Nesse caso, é importante informar ao R que essa variável deve ser tratada como uma variável categórica em vez de contínua.

As variáveis de caractere são diferentes das variáveis de fator, pois não são apenas rótulos para categorias. Um exemplo de variável de caractere seria uma variável que armazena as respostas dos entrevistados para uma pergunta em aberto. Aqui, o conteúdo real é importante (nós nos importamos se alguém descreve sua estadia no Airbnb como muito boa ou excelente ou outra coisa).

No conjunto de dados do airbnb, os room\_id não são rigorosamente determinados de antemão, mas definitivamente são rótulos e não devem ser tratados como números. Por isso, pedimos para o R convertê-los em fatores. Vamos dar uma olhada no conjunto de dados do airbnb novamente para verificar se o tipo dessas variáveis mudou após fatorar:

```{r}
str(airbnb)
```

Vemos que o tipo de room\_id e host\_id agora é fct (fator).

### Transformações numéricas {.unlisted .unnumbered}

Vamos dar uma olhada nas classificações das acomodações:

```{r}
# Uso a funcao head para garantir que o R mostre apenas as primeiras classificacoes.
# Caso contrario, teremos uma lista muito longa de classificacoes..
head(airbnb$overall_satisfaction)
```

Vemos que as classificações estão em uma escala de 0 a 5. Se preferirmos ter classificações em uma escala de 0 a 100, poderíamos simplesmente multiplicar as classificações por 20:

```{r}
airbnb$overall_satisfaction_100 <- airbnb$overall_satisfaction * 20 
# Perceba que criamos uma nova variavel overall_satisfaction_100.
# A variavel original overall_satisfaction continua inalterada.


# Você tambem pode inspecionar todo o conjunto de dados com o Visualizador
# e veja se ha uma nova coluna a direita.
head(airbnb$overall_satisfaction_100) 
```

### Transformando variáveis com a função mutate {.unlisted .unnumbered}

Também podemos transformar variáveis com a função mutate:

```{r}
library(dplyr)#pacote da funcao mutate
airbnb <- mutate(airbnb, 
                 room_id_F = factor(room_id), host_id_F = factor(host_id),
                 overall_satisfaction_100 = overall_satisfaction * 20)
```

Isso instrui R a pegar o conjunto de dados do airbnb, criar uma nova variável room\_id\_F que deve ser a fatoração de room\_id, uma nova variável host\_id\_F que deve ser a fatoração de host\_id e uma nova variável overall\_satisfaction\_100 que deve ser a satisfação geral vezes 20. O conjunto de dados com esses mutações (transformações) devem ser atribuídas ao objeto airbnb. Observe que não precisamos usar o operador \$ aqui, porque a função mutate sabe desde seu primeiro argumento (airbnb) onde procurar determinadas variáveis e, portanto, não precisamos especificá-lo posteriormente com airbnb \$. Uma vantagem do uso da função mutate é que ela mantém bem todas as transformações desejadas dentro de um comando. Outra grande vantagem do uso do mutate será discutida na seção sobre o [operador pipe.](https://bookdown.org/content/1340/pipe.html#pipe)

### Incluindo ou excluindo e renomeando variáveis (colunas) {.unlisted .unnumbered}


Se olharmos para os dados, também podemos ver que country é NA, o que significa que não está disponível ou está ausente. city é sempre a Bélgica (o que está errado porque a Bélgica é um país, não uma cidade) e o borought contém as informações da cidade. Vamos corrigir esses erros removendo a variável country de nosso conjunto de dados e renomeando city e borought. Também excluiremos o survey\_id porque essa variável é constante nas observações e não a usaremos no restante da análise:

```{r chunkbugado}
library(dplyr)
#airbnb <- airbnb%>%
#  select(-country, -survey_id) 

# Diga R para remover country & survey_id do quadro de dados do airbnb incluindo um sinal de menos antes dessas variáveis.

airbnb<-airbnb%>%
  select(room_id ,
         host_id,
         room_type,
         country,
         borough,
         neighborhood,
         reviews,
         overall_satisfaction,
         accommodates,
         bedrooms,
         bathrooms,
         price,
         minstay,
         name,
         last_modified,
         latitude,
         longitude,
         location,
         room_id_F,
         host_id_F,
         overall_satisfaction_100)
```

```{r}
# Atribua novamente esse novo quadro de dados ao objeto airbnb.
str(airbnb) # Agora você verá que o country e o survey_id se foram.

#airbnb <- rename(airbnb, country = city, city = borough) 
# Diga ao R para renomear algumas variáveis do quadro de dados do airbnb e reatribuir esse novo quadro de dados ao objeto do airbnb.
# Nota: a sintaxe é um pouco contra-intuitiva: novo nome de variável (country) = nome da variável antiga (city)!
 # country = Bélgica agora e cidade se refere a cidades
```

### Incluindo ou excluindo observações (linhas) {.unlisted .unnumbered}

#### Criando um vetor com c() {.unlisted .unnumbered}

[Mais adiante](https://bookdown.org/content/1340/graphs.html#graphs), faremos um gráfico dos preços do Airbnb nas dez maiores cidades da Bélgica (em termos de população): Bruxelas, Antuérpia, Gent, Charleroi, Liège, Bruges, Namur, Lovaina, Mons e Aalst.

Para isso, precisamos criar um objeto de dados que tenha apenas dados para as dez maiores cidades. Para fazer isso, primeiro precisamos de um vetor com os nomes das dez maiores cidades, para que, na próxima seção, possamos dizer ao R para incluir apenas os dados dessas cidades:

```{r}
topten <- c("Brussel","Antwerpen","Gent","Charleroi","Liege","Brugge","Namur","Leuven","Mons","Aalst") # Cria um vetor com as 10 maiores cidades
topten # Mostra esse vetor.
```


[Lembre-se](https://bookdown.org/content/1340/airbnbdata.html#assignment) de que um vetor é uma estrutura de dados unidimensional (diferente de um quadro de dados que possui duas dimensões, isto é, colunas e linhas). Usamos o operador \c() para criar um vetor que chamamos de topten. \c() é uma abreviação de concatenar, que significa juntar as coisas. O vetor topten é um vetor de strings (palavras). Deve haver aspas entre as strings. Um vetor de números, no entanto, não requer aspas:


```{r}
number_vector <- c(0,2,4,6)
number_vector
```

Qualquer vetor que você criará aparecerá como um objeto no painel Ambiente (janela superior direita).

##### Incluindo ou excluindo observações com a função filter {.unlisted .unnumbered}

Para armazenar apenas os dados das dez maiores cidades, precisamos do operador \%in\% do pacote Hmisc:

```{r}
#install.packages(Hmisc)
library(Hmisc)
```

Agora podemos usar a função de filtro para instruir o R a reter os dados apenas das dez maiores cidades:

```{r}
airbnb.topten <- filter(airbnb, city %in% topten)
# Filtre o quadro de dados do airbnb para manter apenas as cidades no vetor topten.
# Armazene o conjunto de dados filtrado em um objeto chamado airbnb.topten.

# Entao, estamos criando um novo conjunto de dados airbnb.topten, que eh um subconjunto do conjunto de dados airbnb.
# Verifique o painel Ambiente para ver se o conjunto de dados airbnb.topten tem menos observacoes que o conjunto de dados airbnb,
# porque soh possui dados para as dez maiores cidades.

```


## O operador pipe {.unlisted .unnumbered}


### Uma maneira de escrever o código {.unlisted .unnumbered}

Até agora, aprendemos (entre outras coisas) como ler um arquivo .csv e atribuí-lo a um objeto, como transformar variáveis com a função mutate, como remover variáveis (colunas) do nosso conjunto de dados com a função select, como renomear variáveis com a função rename e como remover observações (linhas) do nosso conjunto de dados com a função de filter:


```{r}
#airbnb <- read_csv("tomslee_airbnb_belgium_1454_2017-07-14.csv")
#airbnb <- mutate(airbnb, room_id_F = factor(room_id), host_id_F = factor(host_id), overall_satisfaction_100 = overall_satisfaction * 20)
#airbnb <- select(airbnb, -country, -survey_id)
#airbnb <- rename(airbnb, country = city, city = borough)
#airbnb <- filter(airbnb, city %in% c("Brussel","Antwerpen","Gent","Charleroi","Liege","Brugge","Namur","Leuven","Mons","Aalst")) 
```

Ao ler este código, vemos que em cada linha substituímos o objeto airbnb. Não há nada de fundamentalmente errado com essa maneira de escrever, mas estamos repetindo elementos do código porque as últimas quatro linhas consistem em uma atribuição (airbnb $<-$) e em funções (mutate, select, rename, filter) que têm o mesmo primeiro argumento (o objeto airbnb criado na linha anterior).


#### Uma maneira melhor de escrever seus códigos {.unlisted .unnumbered}

Existe uma maneira mais elegante de escrever código. Envolve um operador chamado _pipe_ ($\%>\%$). Ele nos permite reescrever nossa sequência usual de operações:

```{r}
#airbnb <- read_csv("tomslee_airbnb_belgium_1454_2017-07-14.csv")
#airbnb <- mutate(airbnb, room_id_F = factor(room_id), host_id_F = factor(host_id), overall_satisfaction_100 = overall_satisfaction * 20)
#airbnb <- select(airbnb, -country, -survey_id)
#airbnb <- rename(airbnb, country = city, city = borough)
#airbnb <- filter(airbnb, city %in% c("Brussel","Antwerpen","Gent","Charleroi","Liege","Brugge","Namur","Leuven","Mons","Aalst")) 
```

como:

```{r}
airbnb <- airbnb %>% 
  mutate(room_id_F = factor(room_id), 
         host_id_F = factor(host_id), 
         overall_satisfaction_100 = overall_satisfaction * 20) #%>% 
  #select(-country, -survey_id) %>% 
  #rename(country = city, city = borough) %>% 
  #filter(city %in% c("Brussel","Antwerpen","Gent","Charleroi","Liege","Brugge","Namur","Leuven","Mons","Aalst")) 
```

Isso pode ser lido de maneira natural: “leia o arquivo csv, depois faça a mutação, selecione, renomeie e depois filtre”. Começamos lendo um arquivo .csv. Em vez de armazená-lo em um objeto intermediário, fornecemos como o primeiro argumento para a função mutate usando o _operador pipe_: %$>$%. É uma boa idéia aprender o atalho para %$>$% de cór: *Ctrl $+$ Shift $+$ M*. 

A função mutate usa os mesmos argumentos acima (crie room\_id\_F, que deve ser uma fatoração de room\_id, etc), mas agora não o fazemos precisamos fornecer o primeiro argumento (em qual conjunto de dados queremos que o mutate funcione). O primeiro argumento seria o quadro de dados resultante da leitura do arquivo .csv na linha anterior, mas isso é automaticamente transmitido como primeiro argumento a ser alterado pelo operador pipe. O operador pipe obtém a saída do que está no lado esquerdo do tubo e fornece isso como o primeiro argumento para o que está no lado direito do pipe (ou seja, a próxima linha de código).

Depois de criar novas variáveis com mutate, descartamos algumas variáveis com select. Novamente, a função select usa os mesmos argumentos acima (soltar país e survey\_id), mas não fornecemos o primeiro argumento (de qual conjunto de dados devemos retirar variáveis), porque ele já é fornecido pelo pipe na linha anterior. Continuamos da mesma maneira e renomeamos algumas variáveis com rename e descartamos algumas observações com o filter.

A escrita de código com o operador de pipe explora a estrutura semelhante de mutate, select, rename, filter, que são as funções mais importantes para manipulação de dados. O primeiro argumento para todas essas funções é o quadro de dados no qual ela deve operar. Agora, esse primeiro argumento pode ser deixado de fora, porque é fornecido pelo operador pipe. No restante deste tutorial, escreveremos código usando o operador de pipe, pois melhora consideravelmente a legibilidade do nosso código.


## Agrupando e resumindo {.unlisted .unnumbered}

Vamos trabalhar no conjunto de dados completo novamente. Até agora, seu script deve ficar assim:

```{r}
library(tidyverse)
#setwd("c:/Dropbox/work/teaching/R/data/") # Direciona seu diretorio de trabalho 

airbnb <- airbnb %>% 
  mutate(room_id = factor(room_id), host_id = factor(host_id)) #%>% # Nao criamos uma nova variavel room_id_F, mas substituimos room_id com sua fatoracao. O mesmo para host_id.
  #select(-country, -survey_id) %>% # dropa country e survey_id
  #rename(country = city, city = borough) # renomeia city e borough

# Deixamos de lado a transformacao da overall_satisfaction
# e deixamos de fora o comando filter para garantir que nao retenhamos apenas os dados das dez cidades mais populosas
```

### Tabelas de frequência {.unlisted .unnumbered}

Cada observação em nosso conjunto de dados é uma sala ou quarto; portanto, sabemos que nossos dados contêm informações sobre 17651 quartos. Digamos que queremos saber quantos quartos existem por cidade:

```{r}
airbnb%>%
 group_by(city)%>% # Use a funcao group_by para agrupar o quadro de dados do airbnb (fornecido pelo pipe na linha anterior) por cidade
 summarise(nr_per_city = n()) # Resuma este objeto agrupado (fornecido pelo pipe na linha anterior): peca ao R para criar uma nova variavel nr_per_city que possua o numero de observacoes em cada grupo (cidade)
```

Dizemos ao R para pegar o objeto airbnb, agrupá-lo por cidade e resumi-lo (summarise). O resumo que queremos é o número de observações por grupo. Nesse caso, as cidades formam os grupos. Os grupos sempre serão a primeira coluna em nossa saída. Obtemos o número de observações por grupo com a função n(). Esses números são armazenados em uma nova coluna denominada nr\_per\_city.


Como você pode ver, essas frequências são classificadas em ordem alfabética por cidade. Em vez disso, podemos classificá-los pelo número de quartos por cidade:

```{r}
airbnb %>% 
  group_by(city) %>%
  summarise(nr_per_city = n()) %>%
  arrange(nr_per_city) # Usa a funcao arrange para classificar em uma coluna selecionada
```


Mostra a cidade com o menor número de quartos no topo. Para exibir a cidade com mais quartos no topo, classifique em ordem decrescente:

```{r}
airbnb %>% 
  group_by(city) %>%
  summarise(nr_per_city = n()) %>%
  arrange(desc(nr_per_city)) # Classifica por ordem descendente
```

Você verá que a capital Bruxelas tem mais quartos em oferta, seguidos por Antwerpen e Gent. Observe que isso é muito parecido com trabalhar com a Tabela Dinâmica no Excel. Você poderia ter feito tudo isso no Excel, mas isso tem várias desvantagens, especialmente ao trabalhar com grandes conjuntos de dados como o nosso: você não tem registro do que clicou, de como classificou os dados e do que pode ter copiado ou excluído. No Excel, é mais fácil cometer erros acidentais sem perceber do que no R. No R, você tem seu script, para poder voltar e verificar todas as etapas de sua análise.

Nota: você também poderia ter feito isso sem o operador pipe:

```{r}
airbnb.grouped <- group_by(airbnb, city)
airbnb.grouped.summary <- summarise(airbnb.grouped, nr_per_city = n())
arrange(airbnb.grouped.summary, desc(nr_per_city))
```

Mas espero que você concorde que o código que usa o operador de pipe é mais fácil de ler. Além disso, sem o operador pipe, você acabará criando muitos objetos desnecessários, como airbnb.grouped e airbnb.grouped.summary.

### Estatísticas Descritivas {.unlisted .unnumbered}

Digamos que, além das frequências por cidade, também desejemos o preço médio por cidade. Queremos que isso seja classificado em ordem decrescente pelo preço médio. Além disso, agora queremos armazenar as frequências e médias em um objeto (na seção anterior, não armazenamos a tabela de frequências em um objeto):

```{r}
airbnb.summary <- airbnb %>% # Armazena este resumo em um objeto chamado airbnb.summary.
  group_by(city) %>%
  summarise(nr_per_city = n(), average_price = mean(price)) %>% # Aqui informamos ao R para criar outra variavel chamada average_price que nos fornece a media dos precos por grupo (city)
  
arrange(desc(average_price)) # Agora organiza por average_price e mostra o maior preco praticado dentre os demais

# Veja o painel de Ambiente para visualizar se ha agora um novo objeto chamado airbnb.summary.

# Ao inves de apenas rodar airbnb.summary, 
# Eu o envolvi em um comando de print e defini n como Inf para ver todas as linhas.

print(airbnb.summary, n = Inf) 
```

Talvez surpreendentemente, as três principais cidades mais caras são Bastogne, Philippeville e Verviers. Talvez o preço médio dessas cidades seja alto por causa de discrepâncias. 

Vamos calcular algumas estatísticas mais descritivas para ver se nosso palpite está correto:

```{r}
airbnb %>%
  group_by(city) %>%
  summarise(nr_per_city = n(), 
            average_price = mean(price),
            median_price = median(price), # calcula a mediana dos precos por grupo (city) 
            max_price = max(price)) %>% # calcula o preco maximo por grupo (city)
  arrange(desc(median_price),
          desc(max_price)) # ordena em descendente pela mediana de preco entao pelo preco maximo
```

Vemos que duas das três cidades com o preço médio mais alto (Verviers e Bastogne) também estão entre as cinco principais cidades com as medianas de preços; portanto, o seu preço médio alto não se deve apenas a alguns quartos com preços extremamente altos (embora tenham o preço mais alto, quartos nessas cidades são muito caros).

### Exportando (summaries) dos dados {.unlisted .unnumbered}


Às vezes, você pode querer exportar dados ou um resumo dos dados. Vamos salvar nossos dados ou resumo em um arquivo .csv (no Excel, podemos convertê-lo em um arquivo do Excel, se quisermos):


```{r}
# o primeiro argumento eh o objeto que voce deseja armazenar, o segundo eh o nome que voce deseja atribuir ao arquivo (nao esqueca a extensao .csv)
# use write_csv2 quando voce tiver um computador belga (AZERTY), caso contrário, os números decimais não serão armazenados como números

# armazenamento de dados
write_excel_csv(airbnb, "airbnb.csv")
write_excel_csv2(airbnb, "airbnb.csv")

# armazenamento de summary
write_excel_csv(airbnb.summary, "airbnb_summary.csv")
write_excel_csv2(airbnb.summary, "airbnb_summary.csv")
```

O arquivo será salvo no seu diretório de trabalho.



### Gráficos {.unlisted .unnumbered}

Faremos gráficos dos dados das dez cidades mais populosas da Bélgica. Se você possui o conjunto de dados completo do Airbnb em sua memória (verifique o painel Ambiente), basta filtrá-lo:

```{r}
airbnb.topten <- airbnb %>% 
  filter(city %in% c("Brussel","Antwerpen","Gent","Charleroi","Liege","Brugge","Namur","Leuven","Mons","Aalst")) # lembre-se de que voce tera que carregar o pacote Hmisc para usar o operador %in%.
```

Se você acabou de iniciar uma nova sessão R, também pode reler o arquivo .csv executando o código na seção da seção anterior.

### Diagrama de dispersão (scatterplot) {.unlisted .unnumbered}

Vamos criar um _scatterplot_  dos preços por cidade:


```{r}
ggplot(data = airbnb.topten, mapping = aes(x = city, y = price)) + 
  geom_point()
```

Se tudo correr bem, uma plotagem deve aparecer no canto inferior direito da tela. As figuras são feitas com o comando ggplot. Na primeira linha, você diz ao ggplot quais dados devem ser usados para criar um gráfico e quais variáveis devem aparecer no eixo X e no eixo Y. Dizemos para colocar cidade no eixo X e preço no eixo Y. A especificação do eixo X e do eixo Y sempre deve vir como argumentos para uma função aes, que por sua vez é fornecida como um argumento para a função mapping (mapeamento). Na segunda linha, você diz ao ggplot para desenhar pontos (geom\_point).

Ao criar um gráfico, lembre-se de sempre adicionar um $+$ no final de cada linha de código que compõe o gráfico, exceto o último (adicionar o $+$ no início de uma linha não funcionará).

O gráfico não é muito informativo porque muitos pontos são desenhados um sobre o outro.

### Jitter {.unlisted .unnumbered}

Vamos adicionar o gráfico do tipo jitter aos nossos pontos:

```{r}
ggplot(data = airbnb.topten, mapping = aes(x = city, y = price)) + 
  geom_jitter() # O mesmo codigo de antes mas agora mudamos geom_point para geom_jitter.
```


Em vez de solicitar pontos com geom\_point(), agora solicitamos pontos com jitter adicionado com geom\_jitter(). Jitter é um valor aleatório que é adicionado a cada coordenada X e Y, de modo que os pontos de dados não sejam desenhados um sobre o outro. Observe que fazemos isso apenas para tornar o gráfico mais informativo (compare-o com o gráfico de dispersão anterior, onde muitos pontos de dados são desenhados um sobre o outro); não altera os valores reais em nosso conjunto de dados.


### Histograma {.unlisted .unnumbered}

Ainda não está claro. Parece que a distribuição do preço está correta. Isso significa que a distribuição do preço não é normal. Uma distribuição normal tem dois recursos principais.

Uma primeira característica é que existem mais valores próximos à média do que valores distantes da média.

Em outras palavras, valores extremos não ocorrem com muita frequência.

Uma segunda característica é que a distribuição é simétrica. Em outras palavras, o número de valores abaixo da média é igual ao número de valores acima da média. Em uma distribuição distorcida, existem valores extremos em apenas um lado da distribuição. No caso de inclinação à direita, isso significa que existem valores extremos no lado direito da distribuição.

No nosso caso, isso significa que existem algumas listagens do Airbnb com preços muito altos. Isso aumenta a média da distribuição, de modo que as listagens não sejam mais normalmente distribuídas em torno da média.

Vamos desenhar um histograma dos preços:

```{r}
ggplot(data = airbnb.topten, mapping = aes(x = price)) + # Observe que nao temos mais uma cidade x =. O preco deve estar no eixo X e as frequencias dos precos devem estar no eixo Y
  geom_histogram() # Eixo Y = frequencia dos valores no eixo X
```


De fato, existem alguns preços extremamente altos (em comparação com a maioria dos preços), portanto, os preços estão inclinados à direita. Nota: o stat\_bin() usando compartimentos = 30. Escolha um valor melhor com o aviso de largura de caixa no console que possa ser ignorado com segurança.


### Transformação logarítmica {.unlisted .unnumbered}

Como a variável price está inclinada à direita, podemos transformá-la em log para torná-la mais normal:

```{r}
# No eixo y agora temos log(price, base=exp(1)) ao inves de price. log(price, base=exp(1)) = assuma o log natural, i.e., o log com base = exp(1) = e.

ggplot(data = airbnb.topten, mapping = aes(x = city, y = log(price, base=exp(1)))) + 
  geom_jitter()
```

### Plotando a mediana {.unlisted .unnumbered}

Vamos ter uma idéia melhor da mediana de preço por cidade:

```{r}
ggplot(data = airbnb.topten, mapping = aes(x = city, y = price)) + 
  geom_jitter() +
  stat_summary(fun.y=median, colour="tomato3", size = 4, geom="point")
```

A linha de código para obter a mediana pode ser lida da seguinte forma: stat\_summary solicitará um resumo estatístico. A estatística que queremos é a mediana em uma cor chamada tomato3, com tamanho 4. Ela deve ser representada como um "ponto". Vemos que Bruges é a cidade com o preço mediano mais alto. É muito mais fácil ver isso quando transformamos o preço por log:

```{r}
ggplot(data = airbnb.topten, mapping = aes(x = city, y = log(price, base = exp(1)))) + 
  geom_jitter() +
  stat_summary(fun.y=median, colour="tomato3", size = 4, geom="point")
```

### Plotando a média {.unlisted .unnumbered}

Vamos adicionar a média também, mas com uma cor e forma diferentes da média:

```{r}
ggplot(data = airbnb.topten, mapping = aes(x = city, y = log(price, base = exp(1)))) + 
  geom_jitter() +
  stat_summary(fun.y=median, colour="tomato3", size = 4, geom="point") +
  stat_summary(fun.y=mean,   colour="green",   size = 4, geom="point", shape = 23, fill = "green")
```

O código para obter a média é muito semelhante ao usado para obter a mediana. Simplesmente alteramos a estatística, a cor e adicionamos a forma =23 para obter diamantes em vez de círculos e preencher =green para preencher os diamantes (pontos do gráfico). Vemos que os meios e medianas são bastante semelhantes.

### Salvando imagens {.unlisted .unnumbered}

Podemos salvar esse gráfico em nosso disco rígido. Para fazer isso, clique em Exportar / Salvar como imagem. Se você não alterar o diretório, o arquivo será salvo no seu diretório de trabalho. Você pode redimensionar a plotagem e também fornecer um nome de arquivo significativo - Rplot01.png não será útil quando você tentar encontrar o arquivo posteriormente.

Uma maneira diferente (reproduzível) de salvar seu arquivo é agrupar o código nas funções png() e dev.off():

```{r}
png("price_per_city.png", width=800, height=600) 
# Isso ira preparar o R para salvar o grafico a seguir. 
# Fornece um nome de arquivo e dimensoes para largura e altura da figura em pixels

ggplot(data = airbnb.topten, mapping = aes(x = city, log(price, base = exp(1)))) + 
  geom_jitter() +
  stat_summary(fun.y=mean, colour="green", size = 4, geom="point", shape = 23, fill = "green") # Somente mantivemos a media aqui

dev.off() # Isso dira ao R que terminamos a plotagem e que ela deve salvar a plotagem no disco rigido.
```

Embora o R tenha uma interface não gráfica, ele pode criar gráficos muito bons. Praticamente todos os pequenos detalhes no gráfico podem ser ajustados. Muitos dos gráficos que você vê em "jornalismo de dados" (por exemplo, em [https://www.nytimes.com/](https://www.nytimes.com/) ou em [http://fivethirtyeight.com/](http://fivethirtyeight.com/) são feitos em R.)

















<!--chapter:end:02-IntroaoR.Rmd-->

# Análise básica de dados: analisando dados secundários {.unlisted .unnumbered}

Neste capítulo, analisaremos os dados do Airbnb.com. A introdução tem mais informações sobre esses dados.

## Dados {.unlisted .unnumbered}
### Importação {.unlisted .unnumbered}

Você pode baixar o conjunto de dados clicando com o botão direito do mouse [nesse link](http://users.telenet.be/samuelfranssens/tutorial_data/tomslee_airbnb_belgium_1454_2017-07-14.csv), selecionando “Salvar link como…” (ou algo semelhante) e salvando o arquivo .csv em um diretório no disco rígido. Como mencionado na introdução, é uma boa ideia salvar seu trabalho em um diretório que é automaticamente copiado pelo software de compartilhamento de arquivos. Vamos importar os dados:

```{r}
library(tidyverse)
#setwd("c:/Dropbox/work/teaching/R/") # Ajusta seu diretorio de trabalho

airbnb <- read.csv(file="http://users.telenet.be/samuelfranssens/tutorial_data/tomslee_airbnb_belgium_1454_2017-07-14.csv") %>% 
  mutate(room_id = factor(room_id), host_id = factor(host_id)) %>% 
  select(-country, -survey_id) %>% # dropa country & survey_id, veja a introdução de por que fazemos isso
  rename(country = city, city = borough) # renomeia city & borough, veja a introdução de por que fazemos isso
```
Não se esqueça de salvar seu script no diretório de trabalho.

### Manipulação {.unlisted .unnumbered}

Se você abrir o quadro de dados do airbnb em uma guia do Visualizador, verá que os bathrooms e o minstay são colunas vazias e que o local e last\_modified não são muito informativos. Vamos remover estas variáveis:

```{r}
airbnb <- airbnb %>% 
  select (-bathrooms, -minstay, -location, -last_modified)
```


Agora, dê uma olhada na variável overall\_satisfaction:

```{r}
# use head() para imprimir apenas os primeiros valores de um vetor, para evitar uma lista muito longa
# tail() imprime apenas os últimos valores de um vetor
head(airbnb$overall_satisfaction) 
```


A segunda classificação é zero. Provavelmente, isso significa que a classificação está faltando, em vez de ser realmente zero. Vamos substituir os valores zero na overall\_satisfaction por NA:

```{r}
airbnb <- airbnb %>% 
  mutate(overall_satisfaction = replace(overall_satisfaction, overall_satisfaction == 0, NA)) 
  
# crie uma variavel "nova" overall_satisfaction que seja igual a overall_satisfaction com valores de NA em que overall_satisfaction seja igual a zero.

# Digamos que desejassemos substituir NA por 0, entao o comando se tornaria: substitute(overall_satisfaction, is.na(overall_satisfaction), 0)
# overall_satisfaction == NA nao funciona

head(airbnb$overall_satisfaction)
```

### Mesclando datasets {.unlisted .unnumbered}

Posteriormente, testaremos se o preço está relacionado a determinadas características dos tipos de quartos. As características potencialmente interessantes são: room\_type, city, reviews, overall\_satisfaction, etc. Para torná-lo ainda mais interessante, podemos aumentar os dados, por exemplo, com dados disponíveis publicamente nas cidades. Reuni os tamanhos de população das cidades belgas mais populosas [deste site](https://population.mongabay.com/population/belgium/). Faça o download desses dados [(aqui)](http://users.telenet.be/samuelfranssens/tutorial_data/population.xlsx) e importe-os para o R:

```{r}
#population <- read_excel("population.xlsx","data")


library(readxl)

url<-"http://users.telenet.be/samuelfranssens/tutorial_data/population.xlsx"
population <- tempfile()
download.file(url, population, mode="wb")
population<-read_excel(path = population, sheet = 1)

population
```

Agora, queremos vincular esses dados ao nosso quadro de dados do airbnb. Isso é muito fácil no R (mas é muito difícil, por exemplo, no Excel):

```{r}
airbnb.merged <- left_join(airbnb, population, by = c("city" = "place"))
# o primeiro argumento eh o conjunto de dados que queremos aumentar
# o segundo argumento eh onde encontramos os dados para aumentar o primeiro conjunto de dados com
# o terceiro argumento sao as variaveis que usamos para vincular um conjunto de dados ao outro (cidade eh uma variavel no airbnb, local eh uma variavel na populacao)   
```


Confira as colunas mais relevantes do quadro de dados airbnb.merged:

```{r}
airbnb.merged %>% 
  select(room_id, city, price, population)
```


Vemos que há uma population de colunas em nosso conjunto de dados airbnb.merged. Você também pode ver isso no painel Ambiente: airbnb.merged tem uma variável a mais que airbnb (mas o mesmo número de observações).

Faltam dados para Bruxelas, no entanto. Isso ocorre porque Bruxelas está escrito em holandês no conjunto de dados airbnb, mas em inglês no conjunto de dados da population. 

Vamos substituir Bruxelas por Bruxelas no conjunto de dados da population (e também alterar a ortografia de duas outras cidades) e vincular os dados novamente:


```{r}
population <- population %>% 
  mutate(place = replace(place, place == "Brussels", "Brussel"),
         place = replace(place, place == "Ostend", "Oostende"),
         place = replace(place, place == "Mouscron", "Moeskroen"))

airbnb.merged <- left_join(airbnb, population, by = c("city" = "place"))

airbnb.merged %>% 
  select(room_id, city, price, population)
```

### Recapitulando: importação e manipulação {.unlisted .unnumbered}

Aqui está o que fizemos até agora, em uma sequência ordenada de operações pipe (faça o download dos dados [aqui](http://users.telenet.be/samuelfranssens/tutorial_data/tomslee_airbnb_belgium_1454_2017-07-14.csv) e [aqui](http://users.telenet.be/samuelfranssens/tutorial_data/population.xlsx):

```{r}
#library(tidyverse)
#setwd("c:/Dropbox/work/teaching/R") # Configura seu diretorio de trabalho

#airbnb <- read_csv("tomslee_airbnb_belgium_1454_2017-07-14.csv") %>% 
  #mutate(room_id = factor(room_id), host_id = factor(host_id),
   #      overall_satisfaction = replace(overall_satisfaction, overall_satisfaction == 0, NA)) %>% 
#  select(-country, -survey_id,- bathrooms, -minstay, -location, -last_modified) %>% 
#  rename(country = city, city = borough) 

#population <- read_excel("population.xlsx","data") %>% 
  #mutate(place = replace(place, place == "Brussels", "Brussel"),
        # place = replace(place, place == "Ostend", "Oostende"),
        # place = replace(place, place == "Mouscron", "Moeskroen"))

#airbnb <- left_join(airbnb, population, by = c("city" = "place"))
```

### Amostras independentes: teste $t$ {.unlisted .unnumbered}

Digamos que queremos testar se os preços diferem entre cidades grandes e pequenas. Para fazer isso, precisamos de uma variável que indique se um Airbnb está em uma cidade grande ou pequena. Na Bélgica, consideramos cidades com uma população de pelo menos cem mil como grande:

```{r}



airbnb <- airbnb.merged %>% 
  mutate(large = population > 100000,
        size = factor(large, labels = c("small","large")))

# Nos poderiamos tambem ter escrito: mutate(size = factor(population > 100000, labels = c("small","large)))

# observando a variavel populacao
head(airbnb$population)

# olhando a maior variavel
head(airbnb$large)

# e o tamanho da variavel
head(airbnb$size)
```


No script acima, primeiro criamos uma variável lógica (esse é outro tipo de variável; discutimos outras aqui). Chamamos essa variável de grande e é TRUE quando a população é maior que 100000 e FALSE, se não. Depois, criamos um novo tamanho de variável que é a fatoração de grande porte. Observe que adicionamos outro argumento à função factor, ou seja, labels, para fornecer os valores large de nomes mais intuitivos. FALSE vem em primeiro lugar no alfabeto e obtém o primeiro rótulo pequeno, TRUE fica em segundo lugar no alfabeto e obtém o segundo rótulo grande.

Para saber quais cidades são grandes e quais são pequenas, podemos solicitar frequências de combinações de tamanho (grande versus pequeno) e city (a própria cidade). Aprendemos como fazer isso no capítulo introdutório (consulte as tabelas de frequência e as estatísticas descritivas):

```{r}
airbnb %>% 
  group_by(size, city) %>% 
  summarise(count = n(), 
            population = mean(population)) %>% # Cidades formam os grupos. Portanto, a populacao media de um grupo = a media de observacoes com a mesma populacao, porque elas vem da mesma cidade = a populacao da cidade
  arrange(desc(size), desc(population)) %>% # maior cidade no topo
  print (n = Inf) # mostra a distribuicao completa das frequencias
```


Vemos que algumas cidades têm um valor de NA para tamanho. Isso ocorre porque não temos população para essas cidades (e, portanto, também não sabemos se é uma cidade grande ou pequena). Vamos filtrar essas observações e verificar as médias e os desvios padrão de preço, dependendo do tamanho da cidade:

```{r}
airbnb.cities <- airbnb %>% 
  filter(!is.na(population)) 
# Filtre as observacoes para as quais nao temos a populacao.
# O ponto de exclamacao deve ser lido como NAO. Entao, queremos manter as observacoes para as quais a populacao NAO eh NA.
# Visite https://r4ds.had.co.nz/transform.html#filter-rows-with-filter para conhecer mais sobre operadores logicos (veja secao 5.2.2).

airbnb.cities %>% 
  group_by(size) %>% 
  summarise(mean_price = mean(price),
            sd_price = sd(price),
            count = n())
```

Vemos que os preços são mais altos nas pequenas e nas grandes cidades, mas queremos saber se essa diferença é significativa. Um teste t de amostras independentes pode fornecer a resposta (as listagens nas grandes cidades e as listagens nas pequenas cidades são as amostras independentes), mas precisamos verificar primeiro uma suposição: as variâncias das duas amostras independentes são iguais?

```{r}
#install.packages(car) # Para o teste de igualdade de variancias precisaremos do pacote car.
library(car)

# Teste de Levene para variancias iguais 
# Baixo valor p significa que as variancias nao sao iguais. 
# Primeiro argumento = variavel dependente continua, segundo argumento = variavel independente categorica.

leveneTest(airbnb.cities$price, airbnb.cities$size) 
```


A hipótese nula de variâncias iguais é rejeitada ($p <0,001$), portanto, devemos continuar com um teste $t$ que pressupõe variâncias desiguais:

```{r}
# Teste se os preços médios das cidades grandes e pequenas são diferentes.
# Indique se o teste deve assumir variações iguais ou não (defina var.equal = TRUE para um teste que assume variações iguais).

t.test(airbnb.cities$price ~ airbnb.cities$size, var.equal = FALSE)
```

Você pode relatar o seguinte: “As cidades grandes (M = 85,42, DP = 82,46) tinham um preço mais baixo ($t$ (5762,79) = 12,376, p $<$0,001, variação desigual assumida) do que as cidades pequenas (M = 110,31, DP = 121,63). ”

### ANOVA univariada {.unlisted .unnumbered}

Quando sua variável independente (categórica) possui apenas dois grupos, é possível testar se as médias da variável dependente (contínua) são significativamente diferentes ou não com um teste $t$. Quando sua variável independente possui mais de dois grupos, você pode testar se as médias são diferentes com uma ANOVA.

Por exemplo, digamos que queremos testar se há uma diferença significativa entre os preços médios de casas e apartamentos inteiros, salas privadas e quartos compartilhados. Vamos dar uma olhada nos meios por tipo de quarto:

```{r}
airbnb.summary <- airbnb %>% 
  group_by(room_type) %>% 
  summarise(count = n(), # obtenha as frequencias dos diferentes tipos de quartos 
            mean_price = mean(price), # o preco medio por tipo de quarto
            sd_price = sd(price)) # e o desvio padrao do preco por tipo de quarto

airbnb.summary
```

Também podemos traçar esses meios em um gráfico de barras:

```{r}
# Ao criar um grafico de barras, o conjunto de dados que serve como entrada para o ggplot eh o resumo com os meios, nao o conjunto de dados completo.
# (Eh por isso que salvamos o resumo acima em um objeto airbnb.summary)

ggplot(data = airbnb.summary, mapping = aes(x = room_type, y = mean_price)) + 
  geom_bar(stat = "identity", position = "dodge")
```

Não é de surpreender que casas ou apartamentos inteiros tenham preços mais altos do que quartos particulares, que, por sua vez, têm preços mais altos que quartos compartilhados. Também vemos que há quase o dobro de casas e apartamentos inteiros do que quartos privativos disponíveis e quase não há quartos compartilhados disponíveis. Além disso, o desvio padrão é muito mais alto na categoria de casas ou apartamentos inteiros do que nas categorias de quarto particular ou compartilhado.

Uma ANOVA pode testar se há diferenças significativas nos preços médios por tipo de quarto. Porém, antes de executar uma ANOVA, precisamos verificar se as premissas da ANOVA são atendidas.

### Suposição de normalidade de resíduos {.unlisted .unnumbered}


A primeira suposição é que a variável dependente (price) é normalmente distribuída em cada nível da variável independente (room\_type). Primeiro, vamos inspecionar visualmente se essa suposição será válida:

```{r}
# Ao criar um histograma, o conjunto de dados que serve como entrada para o ggplot eh o conjunto de dados completo, nao o resumo com os meios

ggplot(data = airbnb, mapping = aes(x = price)) + # Queremos price no eixo x.
  facet_wrap(~ room_type) + # Queremos que isso seja dividido por room_type.
  #facet_wrap garantira que o ggplot crie paineis diferentes no seu gráfico.
  geom_histogram() # geom_histogram garante que as frequencias dos valores no eixo X sejam plotadas.
  ## `stat_bin()` using `bins = 30`. Pega o melhor valor com `binwidth`.
```


Vemos que há inclinação correta para cada tipo de quarto. Também podemos testar formalmente, dentro de cada tipo de quarto, se as distribuições são normais com o teste Shapiro-Wilk. Por exemplo, para as quartos compartilhados:

```{r}
airbnb.shared <- airbnb %>% 
  filter(room_type == "Shared room") # reter dados apenas das salas compartilhadas

shapiro.test(airbnb.shared$price)
```

O valor-$p$ deste teste é extremamente pequeno, portanto a hipótese nula de que a amostra provém de uma distribuição normal deve ser rejeitada. Se tentarmos o teste Shapiro-Wilk para os quartos privados:

```{r}
airbnb.private <- airbnb %>% 
  filter(room_type == "Private room") # armazenar dados apenas dos quartos compartilhados
  
#shapiro.test(airbnb.private$price)

## Error in shapiro.test(airbnb.private$price): sample size must be between 3 and 5000
```

Ocorreu um erro ao dizer que o tamanho da amostra é muito grande. Para contornar esse problema, podemos tentar o teste Anderson-Darling do pacote nortest:

```{r}
#install.packages("nortest")
library(nortest)
ad.test(airbnb.private$price)
```

Mais uma vez, rejeitamos a hipótese nula de normalidade. Deixo como exercício para testar a normalidade dos preços de casas e apartamentos inteiros.

Agora que sabemos que a suposição de normalidade é violada, o que podemos fazer? Podemos considerar transformar nossa variável dependente com uma transformação de log:

```{r}
ggplot(data=airbnb, mapping=aes(x=log(price, base = exp (1)))) + # Queremos o preco transformado em log no eixo X.
 facet_wrap(~ room_type) + # Queremos que isso seja dividido por room_type. Facet_wrap garantira que o ggplot crie paineis diferentes no seu grafico.
 geom_histogram() # geom_histogram garante que as frequencias dos valores no eixo X sejam plotadas.
```

Como você pode ver, uma transformação de log normaliza uma distribuição inclinada à direita. Poderíamos então executar a ANOVA na variável dependente transformada em log. No entanto, na realidade, muitas vezes é seguro ignorar violações da suposição de normalidade (a menos que você esteja lidando com pequenas amostras, o que nós não somos). Vamos simplesmente continuar com o preço não transformado como variável dependente.



### Suposição: homogeneidade de variâncias {.unlisted .unnumbered}

Uma segunda suposição que precisamos verificar é se as variações de nosso preço variável dependente são iguais nas categorias de nossa variável independente room\_type. Normalmente, um gráfico boxplot é informativo:

```{r}
ggplot(data = airbnb, mapping = aes(x = room_type, y = price)) + 
  geom_boxplot()
```


Mas, neste caso, os intervalos interquartis (as alturas das caixas), que normalmente nos dariam uma idéia da variação dentro de cada tipo de quarto, são muito estreitos. Isso ocorre porque o intervalo de valores Y a ser plotado é muito amplo devido a alguns valores extremos. Se observarmos os desvios padrão, porém, veremos que estes são muito maiores para todos as salas e apartamentos do que para os quartos privativo e compartilhado:

```{r}
airbnb %>% 
  group_by(room_type) %>% 
  summarise(count = n(), # obtenha as frequencias dos diferentes tipos de quartos
            mean_price = mean(price), # o preco medio por tipo de quarto
            sd_price = sd(price)) # e o desvio padrao do preco por tipo de quarto
```

Também podemos realizar um teste formal de homogeneidade de variâncias. Para isso, precisamos da função leveneTest do pacote car:

```{r}
#install.packages("car") # Para o teste de variancias iguais, precisamos de um pacote chamado car. Instalamos isso antes, portanto, nao eh necessario reinstala-lo se voce ja o tiver feito.

library(car)

#Teste de Levene de variancias iguais.
# Valor baixo de p significa que as variancias nao sao iguais.
# Primeiro argumento = variavel dependente continua, segundo argumento = variavel independente categorica.

leveneTest(airbnb$price, airbnb$room_type) 
```

Como o valor $p$ é extremamente pequeno, rejeitamos a hipótese nula de variâncias iguais. Assim como no pressuposto da normalidade, as violações do pressuposto de variâncias iguais podem, no entanto, ser frequentemente ignoradas e o faremos neste caso.

### ANOVA {.unlisted .unnumbered}

Para realizar uma ANOVA, precisamos instalar alguns pacotes:

```{r}
#install.packages(remotes) #O pacote de controles remotos nos permite instalar pacotes armazenados no GitHub, um site para desenvolvedores de pacotes. 
#install.packages("car") #Também precisaremos do pacote do carro para executar a ANOVA (não é necessário reinstalá-lo se você já tiver feito isso).

library(remotes)
install_github('samuelfranssens/type3anova') # Instala o pacote type3anova. Esta e as etapas anteriores precisam ser executadas apenas uma vez.

library(type3anova) # Carregue o pacote type3anova.
```

Agora podemos prosseguir com a ANOVA verdadeira:

```{r}
# Primeiro cria um modelo linear
# A formula lm() toma os argumentos de dados 
# A fórmula tem a seguinte sintaxe: variável dependente ~ variável (s) independente

linearmodel <- lm(price ~ room_type, data=airbnb) 

# Em seguida, peça a saída no formato ANOVA. Isso fornece a soma dos quadrados do Tipo III. 
# Observe que isso é diferente da anova (modelo linear), que fornece a soma dos quadrados do tipo I.

type3anova(linearmodel) 
```


Nesse caso, o valor-p associado ao efeito de room\_type é praticamente 0, o que significa que rejeitamos a hipótese nula de que o preço médio é igual para cada room\_type. Você pode relatar o seguinte: “Houve diferenças significativas entre os preços médios das diferentes tipos de salas ($F (2, 17648) = 533,57, p <0,001$).”



### Teste de Tuckey de diferença significativa verdadeira {.unlisted .unnumbered}

Observe que a ANOVA testa a hipótese nula de que as médias em todos os nossos grupos são iguais. A rejeição desta hipótese nula significa que há uma diferença significativa em pelo menos um dos possíveis pares de médias (ou seja, em casa / apartamento inteiro vs. privado e / ou em casa / apartamento inteiro vs. compartilhado e / ou privado) vs. compartilhado). Para ter uma idéia de qual par de médias contém uma diferença significativa, podemos acompanhar o teste de Tukey, que nos dará todas as comparações pareadas.


O teste de Tukey corrige os valores de p para cima - portanto, é mais conservador decidir que algo é significativo - porque as comparações são post-hoc ou exploratórias:

```{r}
TukeyHSD(aov(price ~ room_type, data=airbnb), 
         "room_type") # O primeiro argumento eh um objeto "aov", o segundo eh a nossa variavel independente.
```

Isso nos mostra que não há diferença significativa no preço médio de quartos compartilhados e privados, mas que quartos compartilhados e quartos particulares diferem significativamente de casas e apartamentos inteiros.


# Regressão Linear {.unlisted .unnumbered}

## Regressão Linear Simples {.unlisted .unnumbered}

Digamos que desejamos prever o preço com base em várias características do quarto. Vamos começar com um caso simples em que prevemos preço com base em um preditor: overal\_satisfaction (satisfação geral). A satisfação geral é a classificação que uma listagem recebe no airbnb.com. Vamos fazer um gráfico de dispersão primeiro:

```{r}
ggplot(data = airbnb, mapping = aes(x = overall_satisfaction, y = price)) +
  geom_jitter() # jitter em vez de pontos, caso contrario, muitos pontos sao desenhados um sobre o outro
```



(Recebemos uma mensagem de erro informando que várias linhas foram removidas. Essas são as linhas com valores ausentes para a overall\_satisfaction, portanto, não há necessidade de se preocupar com essa mensagem de erro. Consulte as [manipulações de dados) para saber por que faltam valores para a overall\_satisfaction.](https://bookdown.org/content/1340/data.html#modelling_manipulations)


Os outliers de preço reduzem a informatividade do gráfico, portanto, vamos transformar a variável price. Também vamos adicionar alguns meios ao gráfico, como aprendemos [aqui](https://bookdown.org/content/1340/graphs.html#graphs), e uma linha de regressão:

```{r}
ggplot(data = airbnb, mapping = aes(x = overall_satisfaction, y = log(price, base = exp(1)))) +
  geom_jitter() + # jitter em vez de pontos, caso contrario, muitos pontos sao desenhados um sobre o outro
  stat_summary(fun.y=mean, colour="green", size = 4, geom="point", shape = 23, fill = "green") + # medias
  stat_smooth(method = "lm", se=FALSE) # reta de regressao
```

Vemos que o preço tende a aumentar um pouco com maior satisfação. Para testar se a relação entre preço e satisfação é realmente significativa, podemos realizar uma regressão simples (simples refere-se ao fato de haver apenas um preditor):

```{r}
linearmodel <- lm(price ~ overall_satisfaction, data = airbnb) # criamos um modelo linear. O primeiro argumento eh o modelo que assume a forma de variavel dependente - variavel (s) independente (s). O segundo argumento sao os dados que devemos considerar.

summary(linearmodel) # solicite um resumo dos resultados desse modelo linear
```


Vemos dois parâmetros neste modelo:


- $\beta_{0}$ ou intercepto (29.75)
- $\beta_{1}$ inclinação de overral\_satisfaction (12.35)


Esses parâmetros têm as seguintes interpretações. A interceptação ($\beta_{0}$) é o preço estimado para uma observação com satisfação geral igual a zero. A inclinação ($\beta_{1}$) é o aumento estimado do preço para cada aumento na satisfação geral. Isso determina a inclinação da linha de regressão ajustada no gráfico. Portanto, para uma listagem com uma satisfação geral de, por exemplo, 3,5, o preço estimado é 29,75 + 3,5× 12,35 = 72,98.

A inclinação é positiva e significativa. Você pode relatar o seguinte: “Havia uma relação significativamente positiva entre preço e satisfação geral ($\beta = 12,35$, t (10585) = 6,63, $p <$0,001). "

Na saída, também obtemos informações sobre o modelo geral.

O modelo vem com um valor F de 43,9 com 1 grau de liberdade no numerador e 10585 graus de liberdade no denominador. Essa estatística F nos diz se nosso modelo com um preditor (overall\_satisfaction) prediz a variável dependente (price) melhor do que um modelo sem preditores (o que simplesmente preveria a média do preço para todos os níveis de satisfação geral). Os graus de liberdade nos permitem encontrar o valor $p$ correspondente ($<$0,001) da estatística F (43,9). Os graus de liberdade no numerador são iguais ao número de preditores em nosso modelo. Os graus de liberdade no denominador são iguais ao número de observações menos o número de preditores menos um. Lembre-se de que temos 10587 observações para as quais temos valores para price e overall\_satisfaction. Como o valor $p$ é menor que 0,05, rejeitamos a hipótese nula de que nosso modelo não prediz melhor a variável dependente do que um modelo sem preditores. Observe que, no caso de regressão linear simples, o valor p do modelo corresponde ao valor $p$ do preditor único. Para modelos com mais preditores, não existe essa correspondência.

Por fim, observe também a estatística do R quadrado do modelo. Esta estatística é igual a 0,004. Essa estatística nos diz quanto da variação na variável dependente é explicada por nossos preditores. Quanto mais preditores você adicionar a um modelo, maior será o R quadrado.

## Correlação {.unlisted .unnumbered}

Observe que na regressão linear simples, a inclinação do preditor é uma função da correlação entre o preditor e a variável dependente. Podemos calcular a correlação da seguinte maneira:

```{r}
# Certifique-se de incluir o argumento use, caso contrario, o resultado sera NA devido aos valores ausentes na overall_satisfaction.
# O argumento use instrui o R para calcular a correlacao com base apenas nas observacoes para as quais temos dados sobre price e overall_satisfaction.

cor(airbnb$price, airbnb$overall_satisfaction, use = "pairwise.complete.obs")
```

Vemos que a correlação é positiva, mas muito baixa (r = 0,064).

Elevando ao quadrado essa correlação, você obterá o R quadrado de um modelo com apenas esse preditor (0,064 × 0,064 = 0,004).

Ao lidar com múltiplos preditores (como na próxima seção), podemos gerar uma matriz de correlação. Isso é especialmente útil ao verificar a multicolinearidade. Digamos que desejamos que as correlações entre, price, overall\_satisfaction, reviews, accommodates:

```{r}
airbnb.corr <- airbnb%>%
   filter(! is.na (overall_satisfaction))%>% # caso contrario, você vera NAs no resultado
   select(price, overall_satisfaction, reviews, accommodates)

cor(airbnb.corr) # obter a matriz de correlacao

cor(airbnb.corr) # obtenha a matriz de correlacao
```

Você pode visualizar facilmente essa matriz de correlação:

```{r}
#install.packages(corrplot) # instala e carrega o pacote corrplot
library(corrplot)

corrplot(cor(airbnb.corr), method = "number", type = "lower", bg = "grey") # apresente numa tabela
```


As cores das correlações dependem de seus valores absolutos.

Você também pode obter valores de p para as correlações ($p <0,05$ indica que a correlação difere significativamente de zero):


```{r}
# O comando para os valores-p eh cor.mtest(airbnb.corr)
# Mas queremos apenas os valores-p, portanto $ p
# E arredondamos para cinco digitos, portanto arredondamos (, 5)

round(cor.mtest(airbnb.corr)$p, 5) 
```

## Regressão linear múltipla, com interação {.unlisted .unnumbered}

Frequentemente, estamos interessados em interações entre preditores (por exemplo, overall\_satisfaction e reviews). Uma interação entre preditores nos diz que o efeito de um preditor depende do nível do outro preditor:


```{r}
# overall_satisfaction + reviews: a interacao nao eh incluida como preditor
# overall_satisfaction * reviews: a interacao entre os dois preditores eh incluida como preditora

summary(lm(formula = price ~ overall_satisfaction * reviews, data = airbnb))
```

Com esse modelo, preço estimado = $\beta_{0} + \beta_{1}\mbox{\textit{overall\_satisfaction}} + \beta_{2}reviews + \beta_{3}\mbox{\textit{overall\_satisfaction}} × reviews$, em que:


- $\beta_{0}$ é o intercepto (48.77)
- controlando todas as outras variáveis em nosso modelo
- $\beta_{2}$ representa a relação entre revisões e preço (-0.99), controlando todas as outras variáveis em nosso modelo
- $\beta_{3}$ é a interação entre satisfação geral e revisões (0.19)


Para um determinado nível de reviews, o relacionamento entre overall\_satisfaction e price pode ser reescrito como:

$$
=[\beta_{0}+\beta_{2}\mbox{reviews}]+(\beta_{1}+\beta_{3}\mbox{reviews})\times\,\,\mbox{overall_satisfaction}
$$

Vemos que tanto a interceptação ($\beta_{0}+\beta_{2}reviews$) e a inclinação ($\beta_{1}+\beta_{3}reviews$) a relação entre overall\_satisfaction} e price} depende de reviews}. No modelo sem interações, apenas a interceptação da relação entre overall\_satisfaction e price dependia de reviews. Como adicionamos ao nosso modelo uma interação entre a overall\_satisfaction e o reviews, a inclinação agora também depende de reviews.

Da mesma forma, para um determinado nível de overall\_satisfaction, o relacionamento entre reviews e price pode ser reescrito como:

$$
=[\beta_{0}+\beta_{1}\mbox{overall_satisfaction}]+(\beta_{2}+\beta_{3}\mbox{overall_satisfaction})\times\,\,\mbox{reviews}
$$

Aqui também vemos que tanto a interceptação quanto a inclinação da relação entre revisões (reviews) e preço (price) dependem da satisfação geral (overall\_satisfaction).

Como dito, quando o relacionamento entre uma variável independente e uma variável dependente depende do nível de outra variável independente, temos uma interação entre as duas variáveis independentes. Para esses dados, a interação é altamente significativa ($p <0,001$). Vamos visualizar essa interação. Nós nos concentramos na relação entre satisfação geral e preço. Planejaremos isso para um nível de comentários que possa ser considerado baixo, médio e alto:

```{r}
airbnb %>% 
  filter(!is.na(overall_satisfaction)) %>% 
  summarise(min = min(reviews),
            Q1 = quantile(reviews, .25), # primeiro quartil
            Q2 = quantile(reviews, .50), # segundo quartil ou mediana
            Q3 = quantile(reviews, .75), # terceiro quartil
            max = max(reviews),
            mean = mean(reviews))
```


e crie grupos com base nesses números:

```{r}
airbnb.reviews <- airbnb %>% 
  filter(!is.na(overall_satisfaction)) %>% 
  mutate(review_group = case_when(reviews <= quantile(reviews, .33) ~ "low",
                                  reviews <= quantile(reviews, .66) ~ "medium",
                                  TRUE                              ~ "high"),
         review_group = factor(review_group, levels = c("low","medium","high")))
```

Por isso, pedimos ao R para criar uma nova variável review\_group que deve ser igual a "low" quando o número de revisões for menor ou igual ao 33º percentil, "medium" quando o número de revisões for menor ou igual ao 66º percentil e "high", caso contrário. Depois, fatoramos a variável review\_group recém-criada e fornecemos um novo argumento, levels, que especifica a ordem dos níveis dos fatores (caso contrário, a ordem seria alfabética: alta, baixa, média). Vamos verificar se o agrupamento foi bem-sucedido:

```{r}
# checagem:
airbnb.reviews %>% 
  group_by(review_group) %>% 
  summarise(min = min(reviews), max = max(reviews))
```


De fato, o número máximo de revisões em cada grupo corresponde aos pontos de corte definidos acima. Agora, podemos solicitar a R um gráfico da relação entre overall\_satisfaction e price para os três níveis de revisão:


```{r}
ggplot(data = airbnb.reviews, mapping = aes(x = overall_satisfaction, y = log(price, base = exp(1)))) + # transformacao log de preco
  facet_wrap(~ review_group) + # peca paineis diferentes para cada grupo de revisao
  geom_jitter() +
  stat_smooth(method = "lm", se = FALSE)
```


Vemos que a relação entre overall\_satisfaction e price é sempre positiva, mas é mais positiva para listagens com muitas críticas do que para listagens com poucas críticas. Pode haver muitas razões para isso. Talvez seja o caso de listagens com críticas positivas aumentarem o preço, mas somente depois de receberem uma certa quantidade de críticas.

Também vemos que as listagens com muitas avaliações quase nunca têm uma classificação de satisfação menor que 3. Isso faz sentido, porque é difícil continuar atraindo pessoas quando a classificação de uma listagem é baixa. Listas com poucas críticas tendem a ter baixos índices de satisfação geral.

Portanto, parece que nossos preditores estão correlacionados: quanto mais avaliações uma listagem tiver, maior será seu índice de satisfação. Isso potencialmente apresenta um problema que discutiremos em uma das próximas seções sobre [multicolinearidade](https://bookdown.org/content/1340/linear-regression.html#multicollinearity)


## Premissas {.unlisted .unnumbered}

Antes de tirar conclusões de uma análise de regressão, é preciso verificar várias suposições. Essas premissas devem ser atendidas independentemente do número de preditores no modelo, mas continuaremos com o caso de dois preditores.

## Normalidade de resíduos {.unlisted .unnumbered}

Os resíduos (a diferença entre os valores observados e os estimados) devem ser normalmente distribuídos. Podemos inspecionar visualmente os resíduos:

```{r}
linearmodel <- lm(price ~ overall_satisfaction * reviews, data = airbnb)
residuals <- as_tibble(resid(linearmodel))  

#Atenção: Chamar `as_tibble ()` em um vetor eh desencorajado, porque eh provavel que o comportamento mude no futuro. Use `enframe (name = NULL)` em seu lugar.
## Este aviso eh exibido uma vez por sessao.

# veja os residuos do modelo linear com resid(linearmodel) 
# e mude isso em seu dataframe com as_tibble()

ggplot(data = residuals, mapping = aes(x = value)) + 
  geom_histogram()
```

## Homocedasticidade de resíduos {.unlisted .unnumbered}

Os resíduos (a diferença entre os valores observados e os estimados) devem ter uma variação constante. Podemos verificar isso plotando os resíduos versus os valores previstos:

```{r}
linearmodel <- lm(price ~ overall_satisfaction * reviews, data = airbnb)

 # cria um dataframe (a tibble)
residuals_predicted <- tibble(residuals = resid(linearmodel), # a primeira variavel são residuos, que sao os residuos do nosso modelo linear
                              predicted = predict(linearmodel)) # a segunda variavel eh prevista, quais sao os valores previstos do nosso modelo linear

ggplot(data = residuals_predicted, mapping = aes(x = predicted, y = residuals)) + 
  geom_point()
```

Essa suposição é violada porque, quanto maiores nossos valores previstos, maior a variação que vemos nos resíduos.


## Multicolinearidade {.unlisted .unnumbered}

A multicolinearidade existe sempre que dois ou mais dos preditores em um modelo de regressão são moderadamente ou altamente correlacionados (portanto, é claro que isso não é um problema no caso de regressão simples). Vamos testar a correlação entre overall\_satisfaction e reviews:

```{r}
# Certifique-se de incluir o argumento use, caso contrario, o resultado sera NA devido aos valores ausentes no overall_satisfaction.
# O argumento use instrui o R para calcular a correlacao com base apenas nas observacoes para as quais temos dados sobre price e overall_satisfaction.

cor.test(airbnb$overall_satisfaction,airbnb$reviews, use = "pairwise.complete.obs") # teste para correlacao
```

Nossos preditores são de fato significativamente correlacionados ($p <$0,001), mas a correlação é realmente baixa (0,03). Ao lidar com mais de dois preditores, é uma boa idéia criar uma matriz de correlação.

O problema da multicolinearidade é que ela infla os erros padrão dos coeficientes de regressão. Como resultado, os testes de significância desses coeficientes terão mais dificuldade em rejeitar a hipótese nula. Podemos facilmente ter uma idéia do grau em que os coeficientes são inflados. Para ilustrar isso, vamos estimar um modelo com preditores correlacionados: acomodações (accommodates) e preço (price) ($r = 0,56$).

```{r}
linearmodel <- lm(overall_satisfaction ~  accommodates * price, data = airbnb)
summary(linearmodel)
```

Vemos que todos os preditores são significativos. Vamos dar uma olhada nos fatores de inflação da variância:

```{r}
library(car) # a funcao vif eh do pacote cars

vif(linearmodel)
```
Os fatores VIF informam até que ponto os erros padrão são inflados. Uma regra prática é que VIFs de 5 e acima indicam inflação significativa.

## Teste $\chi^{2}$ {.unlisted .unnumbered}

Suponha que tenhamos interesse em encontrar uma verdadeira jóia (gem) de uma lista. Por exemplo, estamos interessados em listagens com uma classificação de 5 em 5 e pelo menos 30 avaliações:

```{r}
airbnb <- airbnb %>% 
  mutate(gem = (overall_satisfaction == 5 & reviews>=30), # duas condicoes devem ser atendidas antes de dizer que uma listagem eh uma joia
         gem = factor(gem, labels = c("no gem","gem")))  # de a variavel logica rotulos mais intuitivos
```

Agora, digamos que estamos interessados em saber se é mais provável encontrar "jóias" (gem) em cidades pequenas ou grandes (criamos a variável de tamanho aqui). O teste do qui-quadrado pode fornecer uma resposta a essa pergunta testando a hipótese nula de não haver relação entre duas variáveis categóricas (tamanho da cidade:  large vs. small \& gem: yes vs. no). 

Ele compara a tabela de frequências observada com a tabela de frequências que você esperaria quando não houvesse relação entre as duas variáveis. Quanto mais as tabelas de frequência observada e esperada divergem, maior a estatística qui-quadrado, menor o valor de p e menos provável é que as duas variáveis não sejam relacionadas.

Antes de realizarmos um teste qui-quadrado, lembre-se de que algumas cidades têm um valor em falta para o tamanho porque têm um valor em falta para a population. Vamos filtrar isso primeiro:

```{r}
airbnb.cities <- airbnb %>% 
  filter(!is.na(size)) 

# queremos apenas aquelas observacoes em que tamanho nao eh NA. ! significa 'nao'
# veja https://r4ds.had.co.nz/transform.html#filter-rows-with-filter para mais funcoes logicas (desca ate a secao 5.2.2)
```

Agora, imprima as frequências do tamanho da cidade e combinações de gems:

```{r}
airbnb.cities %>% 
  group_by(size, gem) %>% 
  summarise(count = n())
```

Esta informação está correta, mas o formato em que a tabela é apresentada é um pouco incomum. Gostaríamos de ter uma variável como linhas e a outra como colunas:

```{r}
table(airbnb.cities$size, airbnb.cities$gem)
```


Isso é um pouco mais fácil de interpretar. Uma tabela como essa é frequentemente chamada de tabela cruzada. É fácil pedir porcentagens em vez de contagens:

```{r}
crosstable <- table(airbnb.cities$size, airbnb.cities$gem) #Precisamos salvar a tabela cruzada primeiro.

prop.table(crosstable) # Use a funcao prop.table () para solicitar porcentagens.
```


Com base nessas frequências ou porcentagens, não devemos esperar uma forte relação entre size e gem. Vamos realizar o teste do qui-quadrado para testar nossa intuição:

```{r}
chisq.test(crosstable)
```

O valor da estatística qui é 74,35 e o valor p é praticamente 0, por isso rejeitamos a hipótese nula de nenhum relacionamento. Não é o que esperávamos, mas o valor p é baixo porque nossa amostra é bastante grande (15966 observações). 

Você pode relatar o seguinte: “Havia uma relação significativa entre o tamanho da cidade e se uma listagem era ou não uma jóia ($\chi^{2} (1, N = 15966) = 74,35, p <0,001$), de modo que as cidades grandes (8,05\%) tinham uma porcentagem maior de jóias (raridades) do que as cidades pequenas (4,1\%). 

# Regressão Logística (opcional) {.unlisted .unnumbered}


Às vezes, queremos prever uma variável dependente binária, ou seja, uma variável que pode assumir apenas dois valores, com base em várias variáveis independentes contínuas ou categóricas. Por exemplo, digamos que estamos interessados em testar se uma listagem é ou não uma jóia depende do preço e do tipo de quarto da listagem.

Não podemos usar ANOVA ou regressão linear aqui porque a variável dependente é uma variável binária e, portanto, normalmente não é distribuída. Outro problema com a ANOVA ou regressão linear é que ela pode prever valores que não são possíveis (por exemplo, nosso valor previsto pode ser 5, mas apenas 0 e 1 fazem sentido para essa variável dependente). Portanto, usaremos regressão logística. A regressão logística primeiro transforma a variável dependente Y com a transformação do logit. A transformação do logit usa o logaritmo natural das chances de que a variável dependente seja igual a 1:

$$
probabilidades=\displaystyle\frac{P(Y=1)}{P(Y=0}=\displaystyle\frac{P(Y=1)}{1-P(Y=1)}
$$

então o logit $P(Y=1))=\ln\displaystyle\frac{P(Y-1)}{1-P(Y=1)}$

Isso garante que nossa variável dependente seja normalmente distribuída e não seja restrita a ser 0 ou 1.Isso garante que nossa variável dependente seja normalmente distribuída e não seja restrita a ser 0 ou 1.

Vamos realizar a regressão logística:

```{r}
logistic.model <- glm(gem ~ price * room_type, data=airbnb, family="binomial") # o argumento family = "binomial" diz R para tratar a variavel dependente como uma variavel 0/1
summary(logistic.model) # saida da regressao
```

Vemos que o único preditor marginalmente significativo de uma listagem ser ou não uma jóia é o preço da listagem. Você pode relatar o seguinte: “Controlando o tipo de quarto e a interação entre preço e tipo de quarto, havia uma relação negativa marginalmente significativa entre o preço e a probabilidade de uma listagem ser uma jóia ($\beta$ = -0.0007185, $\chi$ (17645) = -1,783, p = 0,075). 

A interpretação dos coeficientes de regressão na regressão logística é diferente da do caso da regressão linear:

```{r}
summary(logistic.model)
```

O coeficiente de regressão do preço é -0.0007185. Isso significa que um aumento de uma unidade no preço levará a um aumento de -0.0007185 nas chances de log de joia ser igual a 1 (ou seja, de uma listagem sendo uma joia). Por exemplo:

$$
\textrm{logit}(P(Y = 1 | \textrm{price} = 60 )) = \textrm{logit}(P(Y = 1 | \textrm{price} = 59 )) - 0.0007185
$$

$$
\mbox{logit}(P(Y = 1 | \mbox{price} = 60 )) = \mbox{logit}(P(Y = 1 | \mbox{price} = 59 )) - 0.0007185
$$
$$
\Leftrightarrow \textrm{logit}(P(Y = 1 | \textrm{price} = 60 )) - \textrm{logit}(P(Y = 1 | \textrm{price} = 59 )) = -0.0007185
$$
$$
\Leftrightarrow \textrm{ln(probs}(P(Y = 1 | \textrm{price} = 60 )) - \textrm{ln(probs}(P(Y = 1 | \textrm{price} = 59 )) = -0.0007185
$$
$$
\Leftrightarrow \textrm{ln}(\dfrac{ \textrm{probs}(P(Y = 1 | \textrm{price} = 60 )}{\textrm{probs}(P(Y = 1 | \textrm{price} = 59 )}) = -0.0007185
$$

$$
\Leftrightarrow \dfrac{ \textrm{probs}(P(Y = 1 | \textrm{price} = 60 )}{\textrm{probs}(P(Y = 1 | \textrm{price} = 59 )} = e^{-0.0007185}
$$
$$
\Leftrightarrow \textrm{probs}(P(Y = 1 | \textrm{price} = 60 ) = e^{-0.0007185} * \textrm{probs}(P(Y = 1 | \textrm{price} = 59 )
$$



Assim, o coeficiente de regressão em uma regressão logística deve ser interpretado como o aumento relativo nas chances da variável dependente ser igual a 1, para cada aumento de unidade no preditor, controlando todos os outros fatores em nosso modelo. Nesse caso, as probabilidades de uma listagem ser uma gema devem ser multiplicadas por $ e ^ {0,0007185} = 0,999 $ ou diminuídas em 0,1\%, para cada aumento de preço unitário. Em outras palavras, listagens mais caras têm menos probabilidade de serem jóias. No exemplo específico acima, as chances de ser uma joia de uma listagem com preço de 60 são $e^{(-0.0007185×5)}= 0,996$ vezes a chance de ser uma jóia de uma listagem com preço de 59.

## Medindo o ajuste de uma regressão logística: porcentagem classificada corretamente {.unlisted .unnumbered}


Nosso modelo usa o preço e o tipo de quarto da listagem para prever se a listagem é uma joia ou não. Ao fazer previsões, é natural nos perguntarmos se nossas previsões são boas. Em outras palavras, usando preço e tipo de quarto, com que frequência prevemos corretamente se uma listagem é uma jóia ou não? Para ter uma idéia da qualidade de nossas previsões, podemos pegar o preço e o tipo de quarto das listagens em nosso conjunto de dados, prever se as listagens são gemas e comparar nossas previsões com o status real da gema das listagens. Vamos primeiro fazer as previsões:

```{r}
airbnb <- airbnb %>% 
  mutate(prediction = predict(logistic.model, airbnb))
# Crie uma nova coluna chamada previsao no quadro de dados do airbnb e armazene nela a previsao,
# baseado em logistic.model, para os dados do airbnb
# De uma olhada nessas previsoes:
head(airbnb$prediction)
# Compare com as observacoes:
head(airbnb$gem)

```




Você vê o problema? As previsões são logits, ou seja, logaritmos de chances de que as listagens sejam gemas, mas as observações simplesmente nos dizem se uma listagem é uma gema ou não. Para uma comparação significativa entre previsões e observações, precisamos transformar os logits em uma decisão: gema ou não gema. É fácil transformar logits em probabilidades usando o exponencial do logit. A relação entre probabilidades e probabilidades é a seguinte:

$$
\textrm{probs} = \dfrac{P(Y = 1)}{P(Y = 0)} = \dfrac{P(Y = 1)}{1 - P(Y = 1)}
$$
$$
\Leftrightarrow \dfrac{probs}{1 + \dfrac{P(Y = 1)}{1 - P(Y = 1)}} = \dfrac{\dfrac{P(Y = 1)}{1 - P(Y = 1)}}{1 + \dfrac{P(Y = 1)}{1 - P(Y = 1)}}
$$

$$
\Leftrightarrow \dfrac{probs}{1 + probs} = \dfrac{\dfrac{P(Y = 1)}{1 - P(Y = 1)}}{\dfrac{1 - P(Y = 1) + P(Y = 1)}{1 - P(Y = 1)}}
$$

$$
\Leftrightarrow \dfrac{probs}{1 + probs} = \dfrac{P(Y = 1)}{1 - P(Y = 1) + P(Y = 1)}
$$
$$
\Leftrightarrow \dfrac{probs}{1 + probs} = P(Y = 1)
$$
Agora vamos calcular, para cada listagem, a probabilidade de a listagem ser uma jóia (gem):

```{r}
airbnb <- airbnb %>% 
  mutate(prediction.logit = predict(logistic.model, airbnb),
         prediction.odds = exp(prediction.logit),
         prediction.probability = prediction.odds / (1+prediction.odds))

# Inspecionando as probabilidades preditas
head(airbnb$prediction.probability)
```


Os primeiros números são probabilidades muito baixas, mas também existem probabilidades mais altas e todas as previsões estão entre 0 e 1, como deveriam. Agora precisamos decidir qual probabilidade é suficiente para prevermos que uma listagem é uma gem ou não. 


Uma escolha óbvia é uma probabilidade de 0,5: uma probabilidade maior que 0,5 significa que prevemos que é mais provável que uma listagem seja uma gem do que não. Vamos converter probabilidades em previsões:

```{r}
airbnb <- airbnb %>% 
  mutate(prediction = case_when(prediction.probability<=.50 ~ "no gem",
                                prediction.probability> .50 ~ "gem"))

# Inspecinando as predicoes
head(airbnb$prediction)
```


Uma etapa final é comparar previsões com observações:

```{r}
table(airbnb$prediction, airbnb$gem)
```

Normalmente, vemos uma tabela 2x2, mas vemos uma tabela com um valor previsto nas linhas e dois valores observados nas colunas. Isso ocorre porque todas as probabilidades previstas estão abaixo de 0,50 e, portanto, sempre previmos que não há gemas. Vamos reduzir o limite para prever que uma listagem é uma gem:

```{r}
airbnb <- airbnb %>% 
  mutate(prediction = case_when(prediction.probability<=.07 ~ "no gem",
                                prediction.probability> .07 ~ "gem"),
         prediction = factor(prediction, levels = c("no gem","gem")))# verifique se nenhuma joia eh o primeiro nivel do nosso fator

# Observando a tabela
table(airbnb$prediction,airbnb$gem)
```

Podemos ver que, com esta regra de decisão (prever gems sempre que a probabilidade prevista de gems for superior a 7\%), obtivemos 11240 + 326 corretas e 5231 + 854 previsões erradas, que é uma taxa de acerto de (11240 + 326) / (11240 + 326 + 5231 + 854) = 65,5\%.



<!--chapter:end:03-AnaliseBasicadedados.Rmd-->


# Análise Básica de Dados: Experimentos {.unlisted .unnumbered}

Neste capítulo, analisaremos os dados de um experimento que testou se o senso de poder das pessoas afeta sua disposição de pagar (WTP) por produtos relacionados ao status (ou seja, por consumo conspícuo) e se essa relação é diferente quando a WTP desses produtos é visível para os outros versus não.

Os participantes vieram ao nosso laboratório em grupos de oito ou sete. Estavam sentados em frente a um computador em cubículos semi-fechados. Na introdução, os participantes leram que primeiro teriam que preencher um questionário de personalidade e uma pesquisa sobre como eles lidavam com dinheiro. Depois disso, eles teriam que trabalhar juntos em grupos de dois em alguns quebra-cabeças.

A primeira parte da sessão foi um questionário de personalidade avaliando dominância e aspirações de status (Cassidy \& Lynn, 1989; Mead \& Maner, 2012). Os participantes leram 18 declarações e indicaram se cada uma delas se aplicava a elas ou não. Após o preenchimento deste questionário, os participantes foram lembrados de que, no final da sessão, teriam que trabalhar juntos com outro participante em alguns quebra-cabeças. Cada díade consistiria em um gerente e um trabalhador. Os participantes leram que a atribuição a esses papéis foi baseada em seus resultados no questionário de personalidade, mas, na realidade, a atribuição a papéis foi aleatória.

Os participantes na condição de alta potência então leram que eram mais adequados para serem gerentes, enquanto os participantes na condição de baixa potência liam que eram mais adequados para serem trabalhadores (Galinsky, Gruenfeld e Magee, 2003). As instruções deixaram claro que os gerentes teriam mais poder na tarefa de resolver quebra-cabeças do que os trabalhadores (eles poderiam decidir como um bônus em potencial de 20 euros seria dividido entre gerente e trabalhador). Antes de iniciar os quebra-cabeças, no entanto, os participantes foram convidados a participar de um estudo diferente.

Em um estudo ostensivamente diferente, a disposição dos participantes de gastar em produtos conspícuos e discretos foi medida. Na introdução desta parte do experimento, a presença do público foi manipulada. Na condição privada, os participantes foram informados simplesmente de que estávamos interessados em seus padrões de consumo. Eles foram questionados quanto gastariam em dez produtos que diferiam na medida em que poderiam ser usados para sinalizar o status. Os produtos conspícuos ou aprimoradores de status eram: um carro novo, uma casa, viagens, roupas e um relógio de pulso (para homens) ou jóias (para mulheres). Os produtos discretos ou com status neutro eram produtos de higiene pessoal básicos, medicamentos domésticos, despertador de quarto, utensílios de cozinha e limpeza doméstica (Griskevicius, et al., 2007). Os participantes responderam em uma escala de nove pontos, variando de 1: "Eu compraria itens muito baratos" a 9: "Eu compraria itens muito caros".

Na condição pública, os participantes foram informados de que estávamos trabalhando em um site onde as pessoas pudessem se encontrar. Este site nos ajudaria a investigar como as pessoas formam impressões entre si com base nos padrões de consumo. Os participantes leram que primeiro teriam que indicar quanto gastariam em alguns produtos. Suas escolhas seriam resumidas em um perfil. Os outros participantes da sessão teriam que formar impressões sobre eles com base nesse perfil. Depois de ver um exemplo da aparência do perfil, os participantes passaram para a mesma medida de consumo da condição privada.

Em suma, o experimento tem um design 2 (poder: alto vs. baixo) x 2 (público: público vs. privado) x 2 (consumo: conspícuo vs. discreto) com poder e audiência manipulados entre os sujeitos e consumo manipulado entre os sujeitos.

As hipóteses neste experimento foram as seguintes:


- Na condição de privado, esperávamos que os participantes de baixa potência tivessem uma WTP maior do que os participantes de alta potência para produtos visíveis, mas não para produtos discretos. Esse padrão de resultados replicaria os resultados de Rucker e Galinsky (2008).
- Esperávamos que a manipulação pública versus privada reduzisse a WTP para produtos visíveis para participantes de baixa potência, mas não para participantes de alta potência.Não esperávamos um efeito da manipulação pública versus privada na WTP para produtos discretos para participantes de baixa ou alta potência.


Este experimento é descrito com mais detalhes em minha tese de doutorado (Franssens, 2016)


#### Referências {.unlisted .unnumbered}

Cassidy, T., & Lynn, R. (1989). _A multifactorial approach to achievement motivation: The development of a comprehensive measure_. Journal of Occupational Psychology, 62(4), 301-312.

Franssens, S. (2016). _Essays in consumer behavior (Doctoral dissertation)_. KU Leuven, Leuven, Belgium.

Galinsky, A. D., Gruenfeld, D. H., & Magee, J. C. (2003). _From Power to action._ Journal of Personality and Social Psychology, 85(3), 453-466. https://doi.org/10.1037/0022-3514.85.3.453

Griskevicius, V., Tybur, J. M., Sundie, J. M., Cialdini, R. B., Miller, G. F., & Kenrick, D. T. (2007). _Blatant benevolence and conspicuous consumption: When romantic motives elicit strategic costly signals._ Journal of Personality and Social Psychology, 93(1), 85-102. https://doi.org/10.1037/0022-3514.93.1.85


Mead, N. L., & Maner, J. K. (2012). _On keeping your enemies close: Powerful leaders seek proximity to ingroup power threats._ Journal of Personality and Social Psychology, 102(3), 576-591. https://doi.org/10.1037/a0025755


Rucker, D. D., & Galinsky, A. D. (2008). _Desire to acquire: Powerlessness and compensatory consumption._ Journal of Consumer Research, 35(2), 257-267. https://doi.org/10.1086/588569


***

## Dados {.unlisted .unnumbered}

### Importação {.unlisted .unnumbered}

Faça o download dos dados [aqui](http://users.telenet.be/samuelfranssens/tutorial_data/power_conspicuous_consumption.xlsx). Como sempre, salve os dados em um diretório (de preferência um backup automático do software de compartilhamento de arquivos) e inicie seu script carregando o tidyverse e definindo o diretório de trabalho no diretório em que você acabou de salvar seus dados:

```{r}
#library(tidyverse)
#library(readxl) # precisamos deste pacote pois nossos dados estao num arquivo Excel
#setwd("c:/dropbox/work/teaching/R/") # mudando para o nosso diretorio de trabalho

#powercc <- read_excel("power_conspicuous_consumption.xlsx","data") # Importe o arquivo Excel. Perceba que o nome da aba do Excel eh data
```

```{r}
library(readxl)

url<-"http://users.telenet.be/samuelfranssens/tutorial_data/power_conspicuous_consumption.xlsx"
powercc <- tempfile()
download.file(url, powercc, mode="wb")
powercc<-read_excel(path = powercc, sheet = 1)

powercc
```

Não se esqueça de salvar seu script no diretório de trabalho.



### Manipulação {.unlisted .unnumbered}

Temos 39 colunas ou variáveis em nossos dados:


- subject identifica os participantes
- start\_date e end\_date indicam o início e o fim da sessão experimental.
- duration indica a duração da sessão experimental
- finished: os participantes concluíram todo o experimento?
- power (alto vs. baixo) e público (privado vs. público) são as condições experimentais
- group\_size: em grupos de quantos participantes compareceram ao laboratório?
- gender e age do participante
- dominance1, dominance2, etc. são as perguntas que mediram a dominância. Um exemplo é "Eu acho que gostaria de ter autoridade sobre outras pessoas". Os participantes responderam com sim (1) ou não (0).
- sa1, sa2 etc. são as perguntas que medem as aspirações de status. Um exemplo é: "Gostaria de um trabalho importante, onde as pessoas me admirassem". Os participantes responderam com sim (1) ou não (0).
- inconspicuous1, inconspicuous2, etc. contêm a WTP para os produtos inconspicuous. Escala de 1: eu compraria itens muito baratos a 9: eu compraria itens muito caros.
- conspicuous1, conspicuous2, etc. contêm a WTP para os produtos conspícuos. Escala de 1: eu compraria itens muito baratos a 9: eu compraria itens muito caros.
- agree: uma questão exploratória que mede se as pessoas concordam que elas são mais adequadas ao papel de trabalhador ou gerente. Os participantes responderam em uma escala de 1: muito mais adequado para a função de trabalhador (gerente) a 7: muito mais adequado para a função de gerente (trabalhador). Números mais altos indicam concordância com a atribuição de função no experimento.

### Fatorar algumas variáveis {.unlisted .unnumbered}

Após a inspeção dos dados, vemos que o tipo de subject é duplo, o que significa que o subject deve ser fatorado para que seus valores não sejam tratados como números. Também fatoraremos nossas condições experimentais:

```{r}
library(dplyr)
powercc <- powercc %>% # nos criamos o objeto powercc anteriormente 
  mutate(subject = factor(subject),
         power = factor(power, levels = c("low","high")), # note os niveis dos argumentos
         audience = factor(audience, levels = c("private","public"))) # note os niveis dos argumentos
```


Observe que fornecemos novos levels de argumento ao fatorar poder e público. Este argumento especifica a ordem dos levels de um fator. No contexto desse experimento, é mais natural falar sobre o efeito da alta versus baixa potência no consumo do que falar sobre o efeito da baixa versus alta potência no consumo. Portanto, dizemos ao R que o baixo nível de energia deve ser considerado como o primeiro nível. Mais adiante, veremos que o resultado das análises pode ser interpretado como efeitos de alta potência (segundo nível) vs. baixa potência (primeiro nível). O mesmo raciocínio se aplica ao fator público, embora não seja necessário fornecer os níveis para esse fator porque o privado vem antes do público em ordem alfabética. Sua escolha do nível para o primeiro ou nível de referência influencia apenas a interpretação, não o resultado real da análise.

Em uma sessão experimental, o alarme de incêndio disparou e tivemos que sair do laboratório. Vamos remover os participantes que não concluíram a experiência:

```{r}
powercc <- powercc %>% # nos ja criamos o objeto powercc anteriormente
  filter(finished == 1) # somente mantenha as observacoes que sao terminadas ou iguais a 1
```


Observe o dobro $==$ ao testar a igualdade. Confira o livro [R4 Data Science para outros operadores lógicos (role para baixo para chegar à Seção 5.2.2).](https://r4ds.had.co.nz/transform.html#filter-rows-with-filter) 


### Calcular a consistência interna e a média de perguntas que medem o mesmo conceito {.unlisted .unnumbered}

Gostaríamos de calcular a média das perguntas que medem a dominância para obter um único número indicando se o participante tem uma personalidade dominante ou não dominante. Antes de fazer isso, devemos ter uma idéia da consistência interna das perguntas que medem o domínio. Isso nos dirá se todas essas perguntas medem o mesmo conceito. Uma medida da consistência interna é o alfa de Cronbach. Para calcular, precisamos de um pacote chamado psych:

```{r}
#install.packages("psych")
library(psych)
```

Depois que o pacote for carregado, podemos usar a função alfa para calcular o alfa de Cronbach para um conjunto de perguntas:

```{r}
dominance.questions <- powercc %>% 
  select(starts_with("dominance")) # pegue o dataframe powercc e selecione todas as viariaveis com o nome que se inicia com dominancia

alpha(dominance.questions) # calcula o alfa de cronbach para essas variaveis

# Observe que também poderíamos ter escrito isso da seguinte maneira:
# powercc %>% select(starts_with("dominance")) %>% cronbach()
```

Isso produz muita saída. Em raw\_alpha, vemos que o alfa é 0,69, que fica no lado inferior (0,70 é geralmente considerado o mínimo necessário), mas ainda está ok. A tabela abaixo nos diz qual seria o alfa se retirássemos uma pergunta de nossa medida. A queda da dominance6 aumentaria o alfa para 0,7. Comparado ao alfa original de 0,69, esse aumento é pequeno e, portanto, não perdemos a dominance6. Se houvesse uma pergunta com um alto "alfa se descartado", isso indicaria que esta pergunta está medindo algo diferente das outras perguntas. Nesse caso, você pode considerar remover esta pergunta da sua medida.

Podemos proceder calculando a média das respostas sobre a questão do domínio:


```{r}
powercc <- powercc %>% 
  mutate(dominance = (dominance1+dominance2+dominance3+dominance4+dominance5+dominance6+dominance7)/7,
         cc = (conspicuous1+conspicuous2+conspicuous3+conspicuous4+conspicuous5)/5,
         icc = (inconspicuous1+inconspicuous2+inconspicuous3+inconspicuous4+inconspicuous5)/5) %>% 
  select(-starts_with("sa"))
```

Também calculei a média das perguntas sobre consumo conspícuo e consumo discreto, mas não sobre as aspirações de status porque o alfa de Cronbach era muito baixo. Excluí as perguntas sobre aspirações de status do conjunto de dados. Deixo como um exercício verificar os alfa de Cronbach de cada um desses conceitos (faça isso antes de excluir as perguntas sobre as aspirações de status, é claro).

### Recapitulando: importando e manipulando {.unlisted .unnumbered}

Aqui está o que fizemos até agora, em uma sequência ordenada de operações canalizadas (faça o [download](http://users.telenet.be/samuelfranssens/tutorial_data/power_conspicuous_consumption.xlsx) dos dados aqui}):

```{r}
#library(tidyverse)
#library(readxl)
#setwd("c:/dropbox/work/teaching/R/") # mudando para seu proprio diretorio

#powercc <- read_excel("power_conspicuous_consumption.xlsx","data")  %>%
 # filter(finished == 1) %>% 
  #mutate(subject = factor(subject),
   #      power = factor(power, levels = c("low","high")),

    #     audience = factor(audience, levels = c("private","public")),
     #    dominance = (dominance1+dominance2+dominance3+dominance4+dominance5+dominance6+dominance7)/7,
     #    cc = (conspicuous1+conspicuous2+conspicuous3+conspicuous4+conspicuous5)/5,
      #   icc = (inconspicuous1+inconspicuous2+inconspicuous3+inconspicuous4+inconspicuous5)/5) %>% 
  # select(-starts_with("sa"))
```

## Teste $t$ {.unlisted .unnumbered}
### Teste $t$ para amostras independentes {.unlisted .unnumbered}

Digamos que queremos testar se homens e mulheres diferem no grau em que são dominantes. Vamos criar um boxplot primeiro e depois verificar as médias e os desvios padrão:

```{r}
library(ggplot2)
ggplot(data = powercc, mapping = aes(x = gender, y = dominance)) + 
  geom_boxplot()
```

```{r}
powercc %>% 
  group_by(gender) %>% 
  summarise(mean_dominance = mean(dominance),
            sd_dominance = sd(dominance))
```

Os homens pontuam um pouco mais alto que as mulheres, mas queremos saber se essa diferença é significativa. Um teste $t$ de amostras independentes pode fornecer a resposta (os homens e as mulheres em nosso experimento são amostras independentes), mas precisamos verificar primeiro uma suposição: as variações das duas amostras independentes são iguais?

```{r}
#install.packages("car") # para o teste de variancias iguais, precisamos de um pacote chamado car
library(car)

# Teste Levene para igualdade de variancias.
# Baixo valor-p indica que as variancias nao sejam iguais.
# Primeiro argumento = variavel dependente continua, segundo argumento = variavel independente categorica

leveneTest(powercc$dominance, powercc$gender) 

```


A hipótese nula de variâncias iguais não é rejeitada ($p = 0,14$), para que possamos continuar com um teste $t$ que assume variâncias iguais:


```{r}
# Teste se os meios de dominancia diferem entre os sexos.
# Indique se o teste deve assumir variacoes iguais ou nao (define var.equal = FALSE para um teste que nao assume variacoes iguais).

t.test(powercc$dominance ~ powercc$gender, var.equal = TRUE) 
```

Você pode relatar o seguinte: “Homens ($M = 0,65, DP = 0,3$) e mulheres ($M = 0,61, DP = 0,25$) não diferiram no grau em que se classificaram como dominantes ($t (141) = -0,69 , p = 0,49$). "

### Teste $t$ para amostras dependentes {.unlisted .unnumbered}

Digamos que queremos testar se as pessoas estão mais dispostas a gastar em itens conspícuos do que em itens discretos. Vamos verificar os meios e os desvios padrão primeiro:

```{r}
powercc %>% # nao ha necessidade de agrupar! nao estamos dividindo nossa amostra em subgrupos
  summarise(mean_cc = mean(cc), sd_cc = sd(cc),
            mean_icc = mean(icc), sd_icc = sd(icc))
```


As médias são mais altas para produtos conspícuos do que para produtos discretos, mas queremos saber se essa diferença é significativa e, portanto, realizar um teste t de amostras dependentes (cada participante classifica produtos conspícuos e discretos, portanto, essas classificações são dependentes):

```{r}
t.test(powercc$cc, powercc$icc, paired = TRUE) # Teste se as medias de cc e icc sao diferentes. Indique que este eh um teste t de amostras dependentes com emparelhado = TRUE.
```

Você pode relatar o seguinte: “As pessoas indicaram que estavam dispostas a pagar mais ($t (142) = 25.064$, $p <0,001$) por produtos conspícuos ($M = 6,01, DP = 1,05$) do que por produtos discretos ($M = 3,6, DP = 0,99$). ”

### Teste $t$ para amostra única {.unlisted .unnumbered}

Digamos que queremos testar se a disposição média de pagar pelos itens visíveis foi significativamente maior que 5 (o ponto médio da escala):

```{r}
t.test(powercc$cc, mu = 5) # Indique a variavel cuja media queremos comparar com um valor especifico (5).
```

Na verdade, é significativamente maior que 5. Você pode relatar o seguinte: "A WTP média para produtos conspícuos (M = 6,01, DP = 1,05) estava significativamente acima de 5 (t (142) = 11,499, $p <0,001$)."

### ANOVA Bivariada {.unlisted .unnumbered}

Quando você lê um artigo acadêmico que relata um experimento, a seção de resultados geralmente começa com uma discussão dos principais efeitos dos fatores experimentais e sua interação, conforme testado por uma Análise de variância ou ANOVA. Vamos nos concentrar primeiro na WTP para produtos que melhoram o status. (Consideramos brevemente a WTP para produtos com status neutro em nossa discussão sobre testes t de amostras dependentes e discutiremos mais detalhadamente na seção ANOVA de medidas repetidas.)

Vamos inspecionar algumas estatísticas descritivas primeiro. Gostaríamos de ver a WTP média para produtos visíveis, o desvio padrão dessa WTP e o número de participantes em cada célula experimental. Já aprendemos como fazer isso no capítulo introdutório (consulte as tabelas de frequência e as estatísticas descritivas):

```{r}
powercc.summary <- powercc %>% # estamos atribuindo o resumo a um objeto, porque precisaremos desse resumo para fazer um grafico de barras
  group_by(audience, power) %>% # agora agrupamos por duas variaveis
  summarise(count = n(),
            mean = mean(cc),
            sd = sd(cc))

powercc.summary
```


Vemos que a diferença na WTP entre os participantes de alta e baixa potência é maior na condição privada do que na pública.

Vamos resumir esses resultados em um gráfico de caixa:

```{r}
# ao criar um grafico de caixa, o conjunto de dados que serve como entrada para o ggplot eh o conjunto de dados completo, nao o resumo com os meios
ggplot(data = powercc, mapping = aes(x = audience, y = cc, fill = power)) + 
  geom_boxplot() # argumentos sao os mesmos que para geom_bar, mas agora fornecemos a geom_boxplot
```

Os gráficos boxplot são informativos porque nos dão uma idéia da mediana e da distribuição dos dados. No entanto, em trabalhos acadêmicos, os resultados das experiências são tradicionalmente resumidos em gráficos de barras (mesmo que os gráficos boxplots sejam mais informativos).

```{r}
# ao criar um grafico de barras, o conjunto de dados que serve como entrada para o ggplot eh o resumo com os meios, nao o conjunto de dados completo
ggplot(data = powercc.summary, mapping = aes(x = audience, y = mean, fill = power)) +
  geom_bar(stat = "identity", position = "dodge")
```

Para criar um gráfico de barras, informamos ao ggplot as variáveis que devem estar no eixo X e no eixo Y e também usamos cores diferentes para diferentes níveis de potência com o comando fill. Então pedimos um gráfico de barras com geom\_bar. stat = "identity" e position = "dodge" devem ser incluídos como argumentos para geom\_bar, mas uma discussão sobre esses argumentos está além do escopo deste tutorial.

Antes de prosseguirmos com a ANOVA, devemos verificar se suas premissas são atendidas. Discutimos as suposições da ANOVA anteriormente, mas ignoramos aqui.

Para realizar uma ANOVA, precisamos instalar alguns pacotes:

```{r}
#install.packages(remotes) # O pacote de controles remotos nos permite instalar pacotes armazenados no GitHub, um site para desenvolvedores de pacotes.
#install.packages("car") # Tambem precisaremos do pacote do carro para executar a ANOVA (não é necessário reinstala-lo se você já tiver feito isso).

library(remotes)
install_github('samuelfranssens/type3anova') # Instale o pacote type3anova. Esta e as etapas anteriores precisam ser executadas apenas uma vez.

library(type3anova) # Carrega o pacote type3anova 
```

Agora podemos prosseguir com a ANOVA real:

```{r}
# Cria um modelo linear primeiramente
# A funcao lm() toma os dados e os argumentos
# A formula tem a seguinte sintaxe: variavel dependente  ~ variavel independente
linearmodel <- lm(cc ~ power*audience, data=powercc) 
# power*audience: interacao eh calculada
# power+audience: interacao nao eh incluida

type3anova(linearmodel) # Em seguida, peça a saída no formato ANOVA. Isso fornece a soma dos quadrados do Tipo III. Observe que isso é diferente da anova (modelo linear), que fornece a soma dos quadrados do tipo I.
```

Você pode relatar esses resultados da seguinte forma: “Nem o principal efeito do poder (F (1, 139) = 2,45, p = 0,12), nem o principal efeito do público (F (1, 139) = 2,3, p = 0,13), nem a interação entre poder e público (F (1, 139) = 1,03, p = 0,31) foi significativa. ”

### Seguindo com contrastes {.unlisted .unnumbered}

Ao realizar um experimento 2 x 2, geralmente fazemos a hipótese de uma interação. Uma interação significa que o efeito de uma variável independente é diferente dependendo dos níveis da outra variável independente. Na seção anterior, a ANOVA nos disse que a interação entre poder e público não era significativa. Isso significa que o efeito de alta versus baixa potência foi o mesmo na condição privada e pública e que o efeito de público versus privado foi o mesmo na condição de alta e baixa potência. Normalmente, a análise para com a não significância da interação. No entanto, faremos os testes de acompanhamento que um faria se a interação fosse significativa.

Vamos considerar o significado da célula novamente:

```{r}
powercc.summary <- powercc %>% 
  group_by(audience, power) %>% 
  summarise(count = n(),
            mean = mean(cc),
            sd = sd(cc))

# ao criar um grafico de barras, o conjunto de dados que serve como entrada para o ggplot eh o resumo com os meios, nao o conjunto de dados completo
ggplot(data = powercc.summary,mapping = aes(x = audience, y = mean, fill = power)) + 
  geom_bar(stat = "identity", position = "dodge")

powercc.summary
```


Queremos testar se a diferença entre alta e baixa potência na condição privada é significativa (baixa: 6,08 vs. alta: 5,63) e se a diferença entre alta e baixa potência na condição pública é significativa (baixa: 6,17 vs. alta : 6,07). Para fazer isso, podemos usar a função de contraste da biblioteca type3anova que instalamos anteriormente. A função de contraste usa dois argumentos: um modelo linear e uma especificação de contraste. O modelo linear é o mesmo de antes:

```{r}
linearmodel <- lm(cc ~ power*audience, data=powercc)
```

Mas, para saber como devemos especificar nosso contraste, precisamos dar uma olhada nos coeficientes de regressão em nosso modelo linear:

```{r}
summary(linearmodel)
```

Temos 4 coeficientes de regressão:


- $\beta_{0}$ ou intercepto
- $\beta_{1}$ ou o coeficiente de powerhigh ou o alto nível do fator de potência (-0,45)
- $\beta_{2}$ ou o coeficiente para audiencepublic ou o nível público do fator público (0,09)
- $\beta_{3}$ ou coeficiente para: powerhigh:audiencepublic (0.35)



Vemos que a estimativa para a interceptação corresponde à média observada na célula privada de baixa potência. Adicione a isso a estimativa de powerhigh e obteremos a média observada na célula privada de alta potência. Adicione à estimativa para a interceptação, a estimativa de audiencepublic e obtenha a média observada na célula pública de baixa potência. Adicione à estimativa para a interceptação, a estimativa de powerhigh, audiencepublic e powerhigh:audiencepublic e obteremos a média observada na célula pública de alto poder. Geralmente visualizo isso em uma tabela (e salvo no meu script) para facilitar a especificação do contraste que quero testar:


Digamos que estamos contrastando energia alta versus baixa na condição privada e testamos se ($\beta_{0}+\beta_{1}$ e $\beta_{0}$ diferem significativamente ou se ($\beta_{0}+\beta_{1})-\beta_{0}=\beta_{1}$ é significativamente diferente de zero. Nós especificamos isso da seguinte maneira:

```{r}
contrast_specification <- c(0, 1, 0, 0) 

# os quatro numeros correspondentes a b0, b1, b2, b3. 
# queremos testar se b1 eh significativo, entao colocamos 1 em 2º lugar (o 1º lugar é para b0)
#contrast(linearmodel, contrast_specification)
## 
## 	 Simultaneous Tests for General Linear Hypotheses
## 
## Fit: aov(formula = linearmodel)
## 
## Linear Hypotheses:
##        Estimate Std. Error t value Pr(>|t|)  
## 1 == 0  -0.4501     0.2579  -1.745   0.0832 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## (Adjusted p values reported -- single-step method)
```


A saída nos diz que a estimativa do contraste é -0,45, que é realmente a diferença entre as médias observadas na condição privada (alta potência: 5,63 vs. baixa potência: 6,08). Você pode relatar esse resultado da seguinte forma: “O efeito de alta (M = 5,63, DP = 0,89) vs. baixa potência (M = 6,08, DP = 1,04) na condição privada foi marginalmente significativo (t (139) = -1,745 , p = 0,083). " Para obter os graus de liberdade, faça um contraste (linearmodel, contrast\_specification)\$df}

Digamos que queremos testar um contraste mais complicado, ou seja, se a média na célula privada de alta potência é diferente da média das médias nas outras células. Estamos testando:


$(\beta_0 + \beta_1)-1/3*[(\beta_0)+(\beta_0 +\beta_2)+(\beta_0+\beta_1+\beta_2+\beta_3)]$


$=\beta_1-1/3*(\beta_1+2\beta_2+\beta_3)$


$=2/3 * \beta_1-2/3 * \beta_2-1/3 * \beta_3$

A especificação de contraste é a seguinte:

```{r}
contrast_specification <- c(0, 2/3, -2/3, -1/3) 
#contrast(linearmodel, contrast_specification)
## 
## 	 Simultaneous Tests for General Linear Hypotheses
## 
## Fit: aov(formula = linearmodel)
## 
## Linear Hypotheses:
##        Estimate Std. Error t value Pr(>|t|)  
## 1 == 0  -0.4764     0.2109  -2.259   0.0254 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## (Adjusted p values reported -- single-step method)
```

Verifique se a estimativa realmente corresponde à diferença entre a média de alta potência, privada e a média das médias nas outras células. Você pode relatar o seguinte: “A célula privada de alta potência tinha uma WTP significativamente menor do que as outras células experimentais em nosso projeto (est = -0,476, t (139) = -2,259, p = 0,025)”.

## Análise de moderação: interação entre variáveis independentes contínuas e categóricas {.unlisted .unnumbered}

Digamos que queremos testar se os resultados do experimento dependem do nível de domínio das pessoas. Em outras palavras, os efeitos do poder e da audiência são diferentes para participantes dominantes versus participantes não dominantes? Ainda em outras palavras, existe uma interação de três vias entre poder, audiência e domínio?

Vamos criar um gráfico primeiro:

```{r}
# Nossa variavel dependente eh o consumo conspicuo, nossa variavel independente (no eixo X) eh a dominancia.
# Entao, estamos tracando a relacao entre dominancia e consumo conspicuo.
# O argumento da cor diz a R que queremos um relacionamento para cada nivel de poder.

ggplot(data = powercc, mapping = aes(x = dominance, y = cc, color = power)) + 
  facet_wrap(~ audience) + # Diga a R que tambem queremos dividir por audiencia.
  geom_jitter() + # Use geom_jitter em vez de geom_point, caso contrario, os pontos serao desenhados um sobre o outro
  geom_smooth(method='lm', se=FALSE) # Desenhe uma linha de regressao linear atraves dos pontos.
```

Este gráfico é informativo. Isso nos mostra que, na condição privada, o efeito da alta versus baixa potência no consumo conspícuo é mais negativo para participantes menos dominantes do que para participantes mais dominantes. Na condição pública, o efeito da alta versus baixa potência no consumo conspícuo não difere entre os participantes menos versus os mais dominantes. Também vemos que, tanto na condição privada quanto na pública, os participantes mais vs. menos dominantes estão mais dispostos a gastar em consumo conspícuo.

Vamos verificar se a interação de três vias é significativa:



```{r}
linearmodel <- lm(cc ~ power * audience * dominance, data = powercc)
type3anova(linearmodel)
```
Somente o efeito da dominância é significativa. Se a interação de três vias fosse significativa, poderíamos acompanhar com mais testes. Por exemplo, poderíamos testar se a interação bidirecional entre dominância e poder é significativa na condição privada, como sugere o gráfico. Para fazer isso, vamos primeiro examinar os coeficientes de regressão do nosso modelo linear:

```{r}
summary(linearmodel)
```
Temos oito termos em nosso modelo:

- $\beta_0$ ou intercepto;
- $\beta_1$ ou o coeficiente para -“powerhigh”-
- $\beta_2$ ou o coeficiente para -“audiencepublic”-
- $\beta_3$ ou o coeficiente para -“dominance”-
- $\beta_4$ ou o coeficiente para -“powerhigh:audiencepublic”-
- $\beta_5$ ou o coeficiente para -“powerhigh:dominance”-
- $\beta_6$ ou o coeficiente para -“audiencepublic:dominance”-
- $\beta_7$ ou o coeficiente para -“powerhigh:audiencepublic:dominance”-


Resultando nessa tabela de coeficientes:

```{r}
#                     private                         public
# low power          [b0] + (b3)                     [b0+b2] + (b3+b6)
# high power         [b0+b1] + (b3+b5)               [b0+b1+b2+b4] + (b3+b5+b6+b7)
```

Como conseguimos essa tabela? Com o modelo linear especificado acima, cada valor estimado de consumo conspícuo (a variável dependente) é uma função da condição experimental do participante e do nível de dominância do participante. Digamos que tenhamos um participante na condição pública de baixa potência com um nível de dominância de 0,5. O valor estimado do consumo conspícuo para esse participante é:


- $\beta_0$ ou intercepto x 1;
- $\beta_1$ \hl{“powerhigh”} (porque o participante não está na condição de alta potência)
- $\beta_2$ \hl{“audiencepublic”} × 1 (na condição pública)
- $\beta_3$ \hl{“dominance”} ×  0.5 (dominance = 0.5)
- $\beta_4$ \hl{“powerhigh:audiencepublic”} × 0 (não no poder superior e na condição pública)
- $\beta_5$ \hl{“powerhigh:dominance”} × 0 (não na condição de alta potência)
- $\beta_6$ \hl{“audiencepublic:dominance”} × 1 x 0,5 (na condição pública e dominância = 0,5)
- $\beta_7$ \hl{“powerhigh:audiencepublic:dominance”} × 0 (não no poder superior e na condição pública)


ou


$$
\beta_0 \times 1 + \beta_1 \times 0 + \beta_2 \times 1 + \beta_3 \times 0.5 + \beta_4 \times 0 + \beta_5 \times 0 + \beta_6 \times 0.5 + \beta_7 \times 0
$$

$= [\beta_0 + \beta_2] + (\beta_3 + \beta_6) \times 0.5$



$= [5.76 + -0.17] + (0.45 + 0.54) * 0.5 = 6.09$



Verifique o gráfico para ver se isso realmente corresponde ao consumo conspícuo estimado de um participante com dominância = 0,5 na célula pública de baixa potência.

A fórmula geral para a célula pública de baixa potência é a seguinte:

$$
[\beta_0 + \beta_2] + (\beta_3 + \beta_6)\times\mbox{dominance}
$$

e podemos obter as fórmulas para as diferentes células de maneira semelhante. Vemos que em cada célula, o coeficiente entre colchetes é o valor estimado na medida de consumo conspícuo para um participante que pontua 0 em dominância. O coeficiente entre parênteses arredondados é o aumento estimado na medida de consumo conspícuo para cada aumento de uma unidade no domínio. Em outras palavras, os coeficientes entre colchetes representam a interceptação e os coeficientes entre colchetes representam a inclinação da linha que representa a regressão do consumo conspícuo (Y) na dominância (X) dentro de cada célula experimental (ou seja, cada potência e público-alvo) combinação).

Um teste da interação entre poder e dominância dentro da condição privada se resumiria a testar se as linhas azul e vermelha no painel esquerdo da figura acima devem ser consideradas paralelas ou não. Se eles são paralelos, o efeito da dominância no consumo conspícuo é o mesmo nas condições de baixa e alta potência. Se eles não são paralelos, o efeito da dominância é diferente nas condições de baixa e alta potência. Em outras palavras, devemos testar se os coeficientes de regressão são iguais dentro de baixa potência, privado e dentro de alta potência, privado. Em outras palavras, testamos se $(\beta_3+\beta_5)-\beta_3=\beta_5$ é igual a zero:

```{r}
# agora temos oito numeros correspondentes aos oito coeficientes de regressao
# queremos testar se b5 eh significativo, entao colocamos 1 em 6º lugar (o 1º lugar é para b0)

contrast_specification <- c(0, 0, 0, 0, 0, 1, 0, 0)

#contrast(linearmodel, contrast_specification)

## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Fit: aov(formula = linearmodel)
## 
## Linear Hypotheses:
##        Estimate Std. Error t value Pr(>|t|)
## 1 == 0    1.502      1.111   1.352    0.179
## (Adjusted p values reported -- single-step method)
```

A interação não é significativa, no entanto. Você pode relatar o seguinte: “Dentro da condição privada, não houve interação entre poder e dominância (t (135) = 1,352, p = 0,18).”

### Análise _spotlight_ {.unlisted .unnumbered}

Em breve.

## Análise ANCOVA {.unlisted .unnumbered}

Descobrimos que nossas condições experimentais não afetam significativamente o consumo conspícuo:

```{r}
linearmodel1 <- lm(cc ~ power*audience, data = powercc)
type3anova(linearmodel1)
```

Por um lado, isso pode significar que simplesmente não há efeitos das condições experimentais no consumo conspícuo. Por outro lado, isso poderia significar que as manipulações experimentais não são fortes o suficiente ou que há muita variação inexplicada em nossa variável dependente (ou ambas). Podemos reduzir a variação inexplicável em nossa variável dependente, no entanto, incluindo uma variável em nosso modelo que suspeitamos estar relacionada à variável dependente. No nosso caso, suspeitamos que a disposição de gastar em consumo discreto (icc) esteja relacionada à disposição de gastar em consumo conspícuo. Embora icc seja uma variável contínua, podemos incluí-la como uma variável independente em nossa ANOVA e isso nos permitirá reduzir a variação inexplicável em nossa variável dependente:

```{r}
linearmodel2 <- lm(cc ~ power*audience + icc, data = powercc)
type3anova(linearmodel2)
```
Vemos que icc está relacionado à variável dependente e, portanto, que a soma dos quadrados dos resíduos desse modelo, ou seja, a variação inexplicada em nossa variável dependente, é menor (130,32) do que a do modelo sem icc (149,93). Os valores p dos fatores experimentais não diminuem, no entanto. Você pode relatar o seguinte: “Controlando a disposição de gastar em consumo discreto, nem o principal efeito do poder (F (1, 138) = 1,99, p = 0,16), nem o principal efeito do público (F (1, 138 ) = 1,21, p = 0,27), nem a interação entre poder e público (F (1, 138) = 2,08, p = 0,15) foi significativa. ”

Chamamos essa análise de ANCOVA porque icc é uma covariável (abrange ou está relacionada à nossa variável dependente).

## Medidas repetidas ANOVA {.unlisted .unnumbered}

Neste experimento, temos mais de uma medida por unidade de observação, ou seja, disposição para gastar em produtos conspícuos e disposição para gastar em produtos discretos. Uma ANOVA de medidas repetidas pode ser usada para testar se os efeitos das condições experimentais são diferentes para produtos conspícuos versus inconspícuos.

Para executar uma ANOVA de medidas repetidas, precisamos reestruturar nosso quadro de dados de amplo a longo. Um amplo quadro de dados possui uma linha por unidade de observação (em nosso experimento: uma linha por participante) e uma coluna por observação (em nosso experimento: uma coluna para os produtos conspícuos e uma coluna para os produtos inconspícuos). Um quadro de dados longo possui uma linha por observação (em nosso experimento: duas linhas por participante, uma linha para o produto conspícuo e uma linha para o produto inconspícuo) e uma coluna extra que indica com que tipo de observação estamos lidando (conspícua ou imperceptível). Vamos converter o quadro de dados e ver como os quadros de dados amplos e longos diferem um do outro.

```{r}
library(tidyr)
powercc.long <- powercc %>% 
  gather(consumption_type, wtp, cc, icc)

# Converter de grande para longo significa que estamos empilhando varias colunas umas sobre as outras. Para isso, precisamos de uma variavel extra para acompanhar qual coluna estamos lidando.
# A funcao de coleta converte conjuntos de dados de amplos para longos.
# O primeiro argumento (consumer_type) nos dira com qual coluna estamos lidando. Essa eh a variavel que armazenara os nomes das colunas que estamos empilhando.
# O segundo argumento (wtp) armazenara as colunas reais empilhadas umas sobre as outras.
# Os argumentos a seguir sao as colunas que queremos empilhar.

# Entao, dizemos ao gather para criar duas novas variaveis: tipo_de_consumo e vontade de pagar, para representar o empilhamento de um determinado numero de colunas.

# Vamos dizer ao R para nos mostrar apenas as colunas relevantes (isto eh apenas para fins de apresentacao):

powercc.long %>%
  select(subject, power, audience, consumption_type, wtp) %>% 
  arrange(subject)
```

Temos duas linhas por assunto, uma coluna de disposição para pagar e outra coluna (tipo de consumo) que indica se está disposta a pagar por produtos visíveis ou imperceptíveis. Compare isso com o amplo conjunto de dados:

```{r}
powercc %>% 
  select(subject, power, audience, cc, icc) %>% 
  arrange(subject) # Mostre somente as colunas relevantes
```


Temos uma linha por assunto e duas colunas, uma para cada tipo de produto.

Agora podemos realizar uma medida repetida ANOVA. Para isso, precisamos do pacote ez.

```{r}
#install.packages("ez") # Precisamos do pacote ez para RM ANOVA
library(ez)
```



Queremos testar se a interação entre poder e público é diferente para produtos conspícuos e inconspícuos. Vamos dar uma olhada em um gráfico primeiro:

```{r}
powercc.long.summary <- powercc.long %>% # para um grafico de barras precisamos de summary primeiro
  group_by(power, audience, consumption_type) %>% # agrupa por tres variaveis independentes
  summarise(wtp = mean(wtp)) # obtem a media de wtp

ggplot(data = powercc.long.summary, mapping = aes(x = audience, y = wtp, fill = power)) + 
  facet_wrap(~ consumption_type) + # create a panel for each consumption type
  geom_bar(stat = "identity", position = "dodge")
```

Agora podemos testar formalmente a interação de três vias:

```{r}
# Especifique os dados, a variavel dependente, o identificador (wid),
# a variavel que representa a condicao dentro dos sujeitos e as variaveis que representam as condicoes entre os sujeitos.

ezANOVA(data = powercc.long, dv = wtp, wid = subject, within = consumption_type, between = power*audience)
```


Vemos nesses resultados que a interação de três vias entre poder, público e tipo de consumo é marginalmente significativa. Você pode relatar o seguinte: "Uma ANOVA de medidas repetidas estabeleceu que a interação de três vias entre poder, público e tipo de consumo era marginalmente significativa (F (1, 139) = 2,98, p = 0,086).














































































































<!--chapter:end:04-AnaliseBasicadedadosExperimentos.Rmd-->

# Análise de componentes principais para mapas perceptivos (office dataset){.unlisted .unnumbered}

Neste capítulo, você aprenderá como realizar uma análise de componentes principais e visualizar os resultados em um mapa perceptivo.

Digamos que tenhamos um conjunto de observações que diferem entre si em várias dimensões; por exemplo, temos várias marcas de uísque (observações) classificadas em vários atributos, como corpo, doçura, sabor, etc. (dimensões). Se algumas dessas dimensões estiverem fortemente correlacionadas, deve ser possível descrever as observações por um número menor (que o original) de dimensões sem perder muita informação. Por exemplo, doçura e frutificação podem ser altamente correlacionadas e, portanto, podem ser substituídas por uma variável. Essa redução de dimensionalidade é o objetivo da análise de componentes principais.

## Dados {.unlisted .unnumbered}
### Importação {.unlisted .unnumbered}

Analisaremos os dados de uma pesquisa na qual os entrevistados foram solicitados a classificar quatro marcas de equipamentos de escritório em seis dimensões. Faça o download dos dados [aqui](http://users.telenet.be/samuelfranssens/tutorial_data/perceptual_map_office.xlsx) e importe-os para o R:

```{r}
library(tidyverse)
library(readxl)


url<-"http://users.telenet.be/samuelfranssens/tutorial_data/perceptual_map_office.xlsx"
office <- tempfile()
download.file(url, office, mode="wb")
office<-read_excel(path = office, sheet = 1)

office

```

### Manipulação {.unlisted .unnumbered}


```{r}
office # visualiza os dados
```

O conjunto de dados consiste em um identificador, a marca do equipamento de escritório (brand}) e a classificação média (entre os entrevistados) de cada marca em seis atributos: escolha grande (large\_choice), preços baixos (low\_prices), qualidade do serviço (service\_quality), qualidade do produto (product\_quality), conveniência (convenience}) e pontuação da preferência (preference\_score). Vamos fatorar o identificador:


```{r}
office <- office %>% 
  mutate(brand = factor(brand))
```


### Recapitulação: importação e manipulação {.unlisted .unnumbered}


Aqui está o que fizemos até agora, em uma sequência ordenada de operações canalizadas/pipe (faça o download dos dados aqui):

```{r}
#library(tidyverse)
#library(readxl)

#office <- read_excel("perceptual_map_office.xlsx","attributes") %>% # 
  #mutate(brand = factor(brand))
```


## Quantos fatores devemos considerar ? {.unlisted .unnumbered}


O objetivo da análise de componentes principais é reduzir o número de dimensões que descrevem nossos dados, sem perder muitas informações. O primeiro passo na análise de componentes principais é decidir o número de componentes ou fatores principais que queremos manter. Para nos ajudar a decidir, usaremos a função PCA do pacote FactoMineR:

```{r}
#install.packages("FactoMineR")
library(FactoMineR)
```

Para poder usar a função PCA, precisamos primeiro transformar o quadro de dados:

```{r}
office.df <- office %>% 
  select(- brand) %>% # A entrada para a analise de componentes principais deve ser apenas as dimensoes, nao o (s) identificador (es); portanto, vamos remover os identificadores.
  as.data.frame() # altere o tipo do objeto para 'data.frame'. Isso eh necessario para a funcao PCA

rownames(office.df) <- office$brand # Defina os nomes das linhas do data.frame para as marcas (isso eh importante mais tarde ao fazer um biplot)
```

Agora podemos prosseguir com a análise de componentes principais:

```{r}
office.pca <- PCA(office.df, graph=FALSE) # Realizar a analise de componentes principais

office.pca$eig # e veja a tabela com informacoes sobre variancia explicada
```

Se olharmos para esta tabela, veremos que dois componentes explicam 98,1\% da variância nas classificações. Isso já é bastante e sugere que podemos fazer com segurança duas dimensões para descrever nossos dados. Uma regra prática aqui é que a variância cumulativa explicada pelos componentes deve ser de pelo menos 70\%.


## Análise de Componentes Principais {.unlisted .unnumbered}


Vamos reter apenas dois componentes ou fatores:

```{r}
office.pca.two <- PCA(office.df, ncp = 2, graph=FALSE) #Peca dois fatores preenchendo o argumento ncp.
```

### Cargas fatoriais {.unlisted .unnumbered}

Agora podemos inspecionar a tabela com as cargas fatoriais:

```{r}
office.pca.two$var$cor %>% #tabela com cargas fatoriais varimax 
#, mas solicite uma rotacao varimax para melhorar a interpretacao
  varimax
```

Essas cargas são as correlações entre as dimensões originais  (large\_choice}, low\_prices}, etc.) e dois fatores são extraídos (Dim.1 and Dim.2}). Nós vemos que low\_prices}, service\_quality}, e product\_quality} pontuação alta no primeiro fator, enquanto large\_choice}, convenience}, e preference\_score} pontuação alta no segundo fator. Poderíamos, portanto, dizer que o primeiro fator descreve o preço e a qualidade da marca e que o segundo fator descreve a conveniência das lojas da marca.

Também queremos saber quanto cada uma das seis dimensões é explicada pelos fatores extraídos. Para isso, precisamos calcular a comunalidade e / ou seu complemento, a singularidade das dimensões:

```{r}
loadings <- as_tibble(office.pca.two$var$cor) %>% # Precisamos capturar os carregamentos como um quadro de dados em um novo objeto. Use as_tibble(), caso contrario, nao podemos acessar os diferentes fatores

  mutate(variable = rownames(office.pca.two$var$cor), # manter o controle dos nomes das linhas (eles sao removidos ao converter para tibble)
         communality = Dim.1^2 + Dim.2^2, 
         uniqueness = 1 - communality) # O operador ^ eleva um valor a uma determinada potencia. Para calcular a comunalidade, precisamos somar os quadrados das cargas em cada fator.
         
loadings

```

A comunalidade de uma variável é a porcentagem da variação dessa variável que é explicada pelos fatores. Seu complemento é chamado de exclusividade. A exclusividade (unicidade) pode ser puro erro de medição ou pode representar algo que é medido de forma confiável por essa variável específica, mas não por nenhuma das outras variáveis. Quanto maior a exclusividade, maior a probabilidade de que seja mais do que apenas erro de medição. Uma exclusividade superior a 0,6 é geralmente considerada alta. Se a exclusividade for alta, a variável não será bem explicada pelos fatores. Vemos que, para todas as dimensões, a comunalidade é alta e, portanto, a singularidade é baixa; portanto, todas as dimensões são bem capturadas pelos fatores extraídos.


### Plotando as cargas fatoriais {.unlisted .unnumbered}


Também podemos plotar as cargas. Para isso, precisamos de outro pacote chamado factoextra:

```{r}
#install.packages(factoextra)
library(factoextra)
fviz_pca_var(office.pca.two, repel = TRUE) # o argumento repel = TRUE garante que o texto seja exibido corretamente no grafico
```

Nós vimos que large\_choice, service\_quality, product\_quality e preference\_score têm pontuações altas no primeiro fator (o Dim1 do eixo X) e essa conveniência tem uma pontuação alta no segundo fator (o Dim2 do eixo Y). Também podemos adicionar as observações (as diferentes marcas) a esse gráfico:


```{r}
fviz_pca_biplot(office.pca.two, repel = TRUE) # tracar as cargas e as marcas juntas em uma trama
```


Isso também é chamado de biplot. Podemos ver, por exemplo, que o OfficeStar tem uma pontuação alta no primeiro fator.





















<!--chapter:end:05-pca.Rmd-->


# Análise de componentes principais para mapas perceptivos (toothpaste dataset) {.unlisted .unnumbered}

Neste capítulo, você aprenderá como realizar uma análise de componentes principais e visualizar os resultados em um mapa perceptivo.

Digamos que tenhamos um conjunto de observações que diferem entre si em várias dimensões; por exemplo, temos várias marcas de uísque (observações) classificadas em vários atributos, como corpo, doçura, sabor, etc. (dimensões ) Se algumas dessas dimensões estiverem fortemente correlacionadas, deve ser possível descrever as observações por um número menor (que o original) de dimensões sem perder muita informação. Por exemplo, doçura e frutificação podem ser altamente correlacionadas e, portanto, podem ser substituídas por uma variável. Essa redução de dimensionalidade é o objetivo da análise de componentes principais.

## Dados {.unlisted .unnumbered}
###  Importação {.unlisted .unnumbered}

Analisaremos os dados de uma pesquisa na qual 60 consumidores foram convidados a responder a seis perguntas sobre pasta de dente. Esses dados foram coletados pelos criadores do Radiant}, que é um pacote do R para análise de negócios que usaremos posteriormente. Faça o download dos dados [aqui](http://users.telenet.be/samuelfranssens/tutorial_data/toothpaste.xlsx) e importe-os para o R:


```{r}
library(tidyverse)
library(readxl)

url<-"http://users.telenet.be/samuelfranssens/tutorial_data/toothpaste.xlsx"
toothpaste <- tempfile()
download.file(url, toothpaste, mode="wb")
toothpaste<-read_excel(path = toothpaste, sheet = 1)

toothpaste

```

###  Manipulação {.unlisted .unnumbered}

```{r}
toothpaste # Visualiza os dados
```

O conjunto de dados consiste em um identificador, consumer}, e as classificações do entrevistado sobre a importância de seis atributos de pasta de dente: prevents\_cavities, shiny\_teeth, strengthens\_gums, freshens\_breath, decay\_prevention\_unimportant, and attractive\_teeth. Nos também temos os respondentes age e gender.

Vamos fatorar o identificador e o gender
:
```{r}
toothpaste <- toothpaste %>% 
  mutate(consumer = factor(consumer),
         gender = factor(gender))
```

###  Recapitulação: importação e manipulação {.unlisted .unnumbered}

Aqui está o que fizemos até agora, em uma sequência ordenada de operações canalizadas (faça o download dos dados [aqui](http://users.telenet.be/samuelfranssens/tutorial_data/toothpaste.xlsx) :

```{r}
#library(tidyverse)
#library(readxl)

#toothpaste <- read_excel("toothpaste.xlsx")
#  mutate(consumer = factor(consumer),
     #    gender = factor(gender))
```


## Quantos fatores devemos considerar ? {.unlisted .unnumbered}

O objetivo da análise de componentes principais é reduzir o número de dimensões que descrevem nossos dados, sem perder muitas informações. O primeiro passo na análise de componentes principais é decidir o número de componentes ou fatores principais que queremos manter. Para nos ajudar a decidir, usaremos a função pre\_factor do pacote radiant:

```{r}
#install.packages("radiant")
library(radiant)

# armazene os nomes das dimensoes em um vetor para que nao tenhamos que digita-las repetidamente

dimensions <- c("prevents_cavities", "shiny_teeth", "strengthens_gums", "freshens_breath", "decay_prevention_unimportant", "attractive_teeth") 

# dica: tambem poderiamos fazer o seguinte:

# dimensions <- toothpaste %>% select(-consumer, -gender, -age) %>% names()

summary(pre_factor(toothpaste, vars = dimensions))

```


Nas Fit measures, vemos que dois componentes explicam 82% da variação nas classificações. Isso já é bastante e sugere que podemos reduzir com segurança o número de dimensões para dois componentes. Uma regra prática aqui é que a variação cumulativa explicada pelos componentes deve ser de pelo menos 70%.


## Análise de Componentes Principais {.unlisted .unnumbered}

Vamos extrair somente dois componentes ou fatores:

```{r}

summary(full_factor(toothpaste, dimensions, nr_fact = 2))
# Peça dois fatores preenchendo o argumento nr_fact.


```

 
###  Cargas fatoriais {.unlisted .unnumbered}

Veja a tabela abaixo do Factor loadings. Essas cargas são as correlações entre as dimensões originais (prevents\_cavities, shiny\_teeth, etc.) e os dois fatores que são retidos (RC1 e RC2). Nós vemos que prevents\_cavities, strengthens\_gums, e decay\_prevention\_unimportant pontuação alta no primeiro fator, enquanto shiny\_teeth, strengthens\_gums, e freshens\_breath pontuação alta no segundo fator. Poderíamos, portanto, dizer que o primeiro fator descreve preocupações relacionadas à saúde e que o segundo fator descreve preocupações relacionadas à aparência.

Também queremos saber quanto cada uma das seis dimensões é explicada pelos fatores extraídos. Para isso, podemos observar a comunalidade das dimensões (cabeçalho: Attribute communalities). A comunalidade de uma variável é a porcentagem da variação dessa variável que é explicada pelos fatores. Seu complemento é chamado de exclusividade (= 1-comunalidade). A exclusividade pode ser puro erro de medição ou pode representar algo que é medido de forma confiável por essa variável específica, mas não por nenhuma das outras variáveis. Quanto maior a exclusividade, maior a probabilidade de que seja mais do que apenas erro de medição. Uma exclusividade superior a 0,6 é geralmente considerada alta. Se a exclusividade for alta, a variável não será bem explicada pelos fatores. Vemos que, para todas as dimensões, a comunalidade é alta e, portanto, a singularidade é baixa; portanto, todas as dimensões são bem capturadas pelos fatores extraídos.

###  Gráfico das cargas fatoriais {.unlisted .unnumbered}

Também podemos traçar as cargas. Para isso, usaremos dois pacotes:

```{r}
#install.packages("FactoMiner")
#install.packages("factoextra")
library(FactoMineR)
library(factoextra)
toothpaste %>% # take dataset
  select(-consumer,-age,-gender) %>% # somente duas dimensoes
  as.data.frame() %>% # converter em um objeto data.frame, caso contrário, o PCA não o aceitará
  PCA(ncp = 2, graph = FALSE) %>% # faça uma análise de componentes principais e retenha 2 fatores
  fviz_pca_var(repel = TRUE) # pegue essa análise e a transforme em uma visualização

```


Vemos que attractive\_teeth, shiny\_teeth, freshens\_breath, têm pontuações altas no segundo fator (o Dim2 do eixo X). prevents\_cavities e strengthens\_gums têm pontuações altas no segundo fator (o eixo Y Dim2) e decay\_prevention\_unimportant tem uma pontuação baixa nesse fator (essa variável mede a importância da prevenção da cárie).

Também podemos adicionar as observações (os diferentes consumidores) a esse gráfico:

```{r}
toothpaste %>% # pega os dados
  select(-consumer,-age,-gender) %>% # obtem as dimensoes somente
  as.data.frame() %>% # converte em data.frame object, caso contratio PCA nao aceita
  PCA(ncp = 2, graph = FALSE) %>% # faz o pca e retem 2 fatores
  fviz_pca_biplot(repel = TRUE) #faz o grafico
```



Isto também é conhecido como biplot.



<!--chapter:end:06-pca2.Rmd-->


# Análise de cluster para segmentação {.unlisted .unnumbered}

Neste capítulo, você aprenderá como realizar uma análise de cluster e uma análise discriminante linear.

Uma análise de cluster trabalha em um grupo de observações que diferem entre si em várias dimensões. Ele encontrará clusters de observações no espaço n-dimensional, de modo que a semelhança de observações dentro de clusters seja a mais alta possível e a similaridade de observações entre clusters seja a mais baixa possível. Você sempre pode executar uma análise de cluster e pode solicitar qualquer número de clusters. O número máximo de clusters é o número total de observações. Nesse caso, cada observação será um cluster, mas isso não seria um cluster muito útil. O objetivo do agrupamento em cluster é encontrar um pequeno número de clusters que possam ser descritos de forma significativa por suas pontuações médias nas n dimensões. Em outras palavras, o objetivo é encontrar diferentes 'perfis' de observações.

A análise discriminante linear tenta prever uma variável categórica com base em várias variáveis independentes contínuas ou categóricas. É semelhante à regressão logística. Nós o usaremos para prever a associação de cluster de uma observação (conforme estabelecido pela análise de cluster) com base em algumas variáveis de segmentação (ou seja, outras informações que temos sobre as observações que não serviram de entrada na análise de cluster).

Analisaremos os dados de 40 entrevistados que avaliaram a importância de vários atributos da loja ao comprar equipamentos de escritório. Usaremos a análise de cluster para encontrar agrupamentos de observações, neste caso, agrupamentos de respondentes. Esses clusters terão perfis diferentes (por exemplo, um cluster pode atribuir importância à política de preços e devoluções, o outro pode atribuir importância à variedade de opções e qualidade de serviço). Em seguida, usaremos a análise discriminante linear para testar se podemos prever a associação ao cluster (ou seja, que tipo de comprador de equipamento de escritório alguém é) com base em várias características dos entrevistados (por exemplo, sua renda).


## Dados {.unlisted .unnumbered}
### Importação {.unlisted .unnumbered}

Analisaremos os dados de uma pesquisa na qual 40 entrevistados foram solicitados a avaliar a importância de vários atributos da loja ao comprar equipamentos. Faça o download dos dados [aqui](http://users.telenet.be/samuelfranssens/tutorial_data/segmentation_office.xlsx) e importe-os para o R:

```{r}
library(tidyverse) 
library(readxl)

url<-"http://users.telenet.be/samuelfranssens/tutorial_data/segmentation_office.xlsx"
equipment <- tempfile()
download.file(url, equipment, mode="wb")
equipment<-read_excel(path = equipment, sheet = 2)

```


### Manipulação {.unlisted .unnumbered}

```{r}
equipment[1:5,]
```

Temos 10 colunas ou variáveis em nossos dados:

- respondent\_id é um identificador para nossas observações
- Os entrevistados classificaram a importância de cada um dos seguintes atributos em uma escala de 1 a 10: variety\_of\_choice, electronic, furniture, quality\_of\_service, low\_prices, return\_policy.
- professional: 1 para profissionais, 0 para não profissionais
- income: expresso em milhares de dólares
- age


A análise de cluster tentará identificar clusters com padrões semelhantes de classificações. A análise discriminante linear preverá a associação do cluster com base nas variáveis de segmentação (professional}, income}, e age}).

Como sempre, vamos fatorar as variáveis que devem ser tratadas como categóricas:

```{r}
equipment <- equipment %>% 
  mutate(respondent_id = factor(respondent_id),
         professional = factor(professional, labels = c("non-professional","professional")))
```

### Recapitulação: importação e manipulação {.unlisted .unnumbered}

Aqui está o que fizemos até agora, em uma sequência ordenada de operações canalizadas/pipe (faça o download dos dados [aqui](http://users.telenet.be/samuelfranssens/tutorial_data/segmentation_office.xlsx)):

```{r}
#library(tidyverse) 
#library(readxl)

#equipment <- read_excel("segmentation_office.xlsx","SegmentationData") %>%
 # mutate(respondent_id = factor(respondent_id),
  #       professional = factor(professional, labels = c("non-professional","professional")))
```

## Análise de Cluster {.unlisted .unnumbered}

Primeiro, realizaremos uma análise hierárquica de cluster para encontrar o número ideal de clusters. Depois disso, realizaremos uma análise de cluster não hierárquica e solicitaremos o número de clusters considerados ideais pela análise de cluster hierárquica. As variáveis que servirão de entrada para a análise de cluster são as classificações de importância dos atributos da loja.

###  Padronizar ou não ? {.unlisted .unnumbered}

A primeira etapa de uma análise de cluster é decidir se padronizará as variáveis de entrada. A padronização não é necessária quando as variáveis de entrada são medidas na mesma escala ou quando as variáveis de entrada são os coeficientes obtidos por uma análise conjunta. Em outros casos, a padronização é necessária.

No nosso exemplo, todas as variáveis de entrada são medidas na mesma escala e, portanto, a padronização não é necessária. Se necessário, isso pode ser feito facilmente com mutate(newvar = scale(oldvar))).

###  Cluster hierárquico {.unlisted .unnumbered}

Em seguida, criamos um novo conjunto de dados que inclui apenas as variáveis de entrada, ou seja, as classificações:

```{r}
cluster.data <- equipment %>% 
  select(variety_of_choice, electronics, furniture, quality_of_service, low_prices, return_policy) # Selecione no conjunto de dados do equipamento apenas as variaveis com classificacoes padronizadas
```

Agora, podemos prosseguir com o cluster hierárquico para determinar o número ideal de clusters:

```{r}
# A funcao dist() cria uma matriz de dissimilaridade do nosso conjunto de dados e deve ser o primeiro argumento para a funcao hclust().
# No argumento do metodo, voce pode especificar o metodo a ser usado para armazenamento em cluster.
hierarchical.clustering <- hclust(dist(cluster.data), method = "ward.D") 
```

A análise de cluster é armazenada no objeto hierarchical.clustering e pode ser facilmente visualizada por um dendograma:

```{r}
plot(hierarchical.clustering)
```


A partir desse dendograma, parece que podemos dividir as observações em dois, três ou seis grupos de observações. Vamos realizar um teste formal, a regra de parada de Duda-Hart, para ver quantos clusters devemos reter. Para isso, precisamos (instalar e) carregar o pacote NbClust}:

```{r}
#install.packages("NbClust")
library(NbClust)
```

A tabela de regras de parada de Duda-Hart pode ser obtida da seguinte maneira:

```{r}
duda <- NbClust(cluster.data, distance = "euclidean", method = "ward.D2", max.nc = 9, index = "duda")
pseudot2 <- NbClust(cluster.data, distance = "euclidean", method = "ward.D2", max.nc = 9, index = "pseudot2")

duda$All.index


pseudot2$All.index

```


A sabedoria convencional para decidir o número de grupos com base na regra de parada de Duda-Hart é encontrar um dos maiores valores de Duda que corresponda a um baixo valor de pseudo-$T^2$. No entanto, você também pode solicitar o número ideal de clusters, conforme sugerido pela regra de parada:

```{r}
duda$Best.nc
```

Nesse caso, o número ideal é três.

## Cluster não-hierárquico {.unlisted .unnumbered}

Agora, realizamos uma análise de cluster não hierárquica na qual solicitamos três clusters (conforme determinado pela análise de cluster hierárquica):

```{r}
# existe um elemento aleatorio na analise de cluster
# isso significa que voce nem sempre obtera a mesma saida toda vez que fizer uma analise de cluster
# se voce deseja obter sempre a mesma saida, eh necessario corrigir o gerador de numeros aleatorios de R com o comando set.seed
set.seed (1)

# o argumento nstart deve ser incluido e definido como 25, mas sua explicacao esta fora do escopo deste tutorial
kmeans.clustering <- kmeans(cluster.data, 3, nstart = 25)
```

Adicione ao conjunto de dados equipament} uma variável que indique a qual cluster uma observação pertence:

```{r}
equipment <- equipment %>% 
  mutate(km.group = factor(kmeans.clustering$cluster, labels=c("cl1","cl2","cl3"))) # Fatore o indicador de cluster a partir do quadro de dados clustering k e adicione-o ao quadro de dados do equipamento.
```

Inspeciona os clusters:
```{r}
equipment %>% 
  group_by(km.group) %>% # agrupado por cluster (km.group)
  summarise(count = n(), 
            variety = mean(variety_of_choice), 
            electronics = mean(electronics), 
            furniture = mean(furniture), 
            service = mean(quality_of_service), 
            prices = mean(low_prices), 
            return = mean(return_policy)) #Em seguida, pergunte pelo número de entrevistados e pelos meios das classificações.

```

Vemos que:

-  o cluster 1 atribui mais importância (do que outros clusters) à qualidade do serviço
-  o cluster 2 atribui mais importância à variedade de opções
-  o cluster 3 atribui mais importância a preços baixos

Também podemos testar se há diferenças significativas entre os clusters, por exemplo, na variedade de opções. Para isso, usamos uma ANOVA unidirecional:

```{r}
# remotes::install_github("samuelfranssens/type3anova") # para instalar o pacote type3anova.
# Voce precisa do pacote de controles remotos para isso e o pacote para carro precisa ser instalado para que o pacote type3anova funcione

library(type3anova)
type3anova(lm(variety_of_choice ~ km.group, data=equipment))

```

Existem diferenças significativas entre os clusters em importância associadas à variedade de opções, e isso faz sentido porque o objetivo da análise de clusters é maximizar as diferenças entre os clusters. Vamos acompanhar isso com o HSD de Tukey para ver exatamente quais meios diferem um do outro:


```{r}
TukeyHSD(aov(variety_of_choice ~ km.group, data=equipment), 
         "km.group") # O primeiro argumento eh um objeto "aov", o segundo eh a nossa variavel independente.
```
Vemos que em todos os meios, a diferença é significativa.

## LDA Canônico {.unlisted .unnumbered}

Na vida real, geralmente não sabemos o que os potenciais compradores consideram importante, mas temos uma ideia, por exemplo, de sua renda, idade e status profissional. Portanto, seria útil testar quão bem podemos prever a associação ao cluster (perfil de classificações de importância) com base nas características dos respondentes (renda, idade, profissional), que também são chamados de variáveis de segmentação. A fórmula preditiva poderia então ser usada para prever a associação ao cluster de novos compradores em potencial. Para encontrar a fórmula correta, usamos a análise discriminante linear (LDA). Mas primeiro vamos dar uma olhada nas médias de income, age, e professional por cluster:

```{r}

equipment %>% 
  group_by(km.group) %>% # Agrupar equipamentos por cluster.
  summarise(income = mean(income), 
            age = mean(age), 
            professional = mean(as.numeric(professional)-1)) 


#visualiza as predicoes

```


Vemos que o cluster 1 e 2 são um tanto semelhantes em termos de renda e idade, mas diferem na medida em que são compostos por profissionais. O cluster 3 difere do cluster 1 e 2 por ser mais jovem e menos rico.

Agora podemos usar o LDA para testar o quão bem podemos prever a associação do cluster com base na renda, idade e profissional:

```{r}
library(MASS) # We need the MASS package. Install it first if needed.

lda.cluster3 <- lda(km.group ~ income + age + professional, data=equipment, CV=TRUE) # CV = TRUE ensures that we can store the prediction of the LDA in the following step
equipment <- equipment %>% 
  mutate(class = factor(lda.cluster3$class, labels = c("lda1","lda2","lda3"))) # Save the prediction of the LDA as a factor. (The predictions are stored in lda.cluster3$class)
```

Vamos ver como o LDA tem se saído bem:

```{r}
ct <- table(equipment$km.group, equipment$class) # how many observations in each cluster were correctly predicted to be in that cluster by LDA?
ct
```

Vemos, por exemplo, que para as 14 observações no cluster 1, o LDA prevê corretamente que 12 estão no cluster 1, mas prediz erroneamente que 2 estão no cluster 2 e 0 estão no cluster 3.

A precisão geral da previsão pode ser obtida da seguinte forma:

```{r}
prop.table(ct) # get percentages

# add the percentages on the diagonal
sum(diag(prop.table(ct))) # Proportion correctly predicted
```


Digamos que desejamos prever a participação no cluster de novas pessoas para as quais temos apenas renda, idade e status profissional, mas não sua participação no cluster. Poderíamos olhar para a fórmula que o LDA derivou dos dados de pessoas para as quais tínhamos membros do cluster:

```{r}
lda.cluster3.formula <- lda(km.group ~ income + age + professional, data=equipment, CV=FALSE) # CV = FALSE ensures that we view the formula that we can use for prediction
lda.cluster3.formula
```

Vemos que o LDA manteve duas dimensões discriminantes (e é aqui que difere da regressão logística, que é unidimensional). A primeira dimensão explica 77,76 por cento da variância em km.group, a segunda dimensão explica 22,24 por cento da variância em km.group. A tabela com coeficientes nos dá a fórmula para cada dimensão:

Pontuação Discriminante 1 = 0,03 × renda + 0,08 x idade + 0,42 × Pontuação profissional e discriminante 2 = -0,04 × renda + 0,04 ×  idade + 2,1 × profissional.

Para atribuir novas observações a um determinado cluster, primeiro precisamos calcular as pontuações discriminantes médias dos clusters e de cada nova observação (preenchendo os valores (médios) de renda, idade e profissional de cada cluster ou observação no discriminante ), calcule as distâncias geométricas entre as pontuações discriminantes das novas observações e as pontuações discriminantes médias dos clusters e, finalmente, atribua cada observação ao cluster que está mais próximo no espaço geométrico. Isso é uma chatice, por isso temos sorte de o R fornecer uma maneira simples de fazer isso.

Vamos criar algumas novas observações primeiro:

```{r}
# the tibble function can be used to create a new data frame
new_data <- tibble(income = c(65, 65, 35, 35), # to define a variable within the data frame, first provide the name of the variable (income), then provide the values
                   age = c(20, 35, 45, 60),
                   professional = c("professional","non-professional","non-professional","professional"))

# check out the new data:
new_data
```
Agora vamos prever a associação do cluster para essas "novas" pessoas:

```{r}
new_data <- new_data %>% 
  mutate(prediction = predict(lda.cluster3.formula, new_data)$class) 
# Create a new column called prediction in the new_data data frame and store in that the prediction, 
# accessed by $class, for the new_data based on the formula from the LDA based on the old data (use the LDA where CV = FALSE).

# have a look at the prediction:
new_data
```















<!--chapter:end:07-Cluster.Rmd-->


# Análise Conjunta {.unlisted .unnumbered}

Neste capítulo, você aprenderá como realizar uma análise conjunta. A análise conjunta começa a partir de uma pesquisa na qual as pessoas avaliam ou escolhem entre produtos (por exemplo, carros) que diferem em vários atributos (por exemplo, segurança, eficiência de combustível, conforto etc.). A partir dessas classificações ou escolhas, a análise determina o valor que as pessoas atribuem aos diferentes atributos do produto (por exemplo, quanto peso as pessoas atribuem à segurança ao escolher entre carros). Essas informações podem ser usadas no desenvolvimento de produtos.

## Dados {.unlisted .unnumbered}
 
###  Importação {.unlisted .unnumbered}

Analisaremos os dados de uma pesquisa em que 15 consumidores foram convidados a avaliar dez sorvetes. Cada sorvete tinha um 'perfil' diferente, ou seja, uma combinação diferente de níveis de quatro atributos: sabor (framboesa, chocolate, morango, manga, baunilha), embalagem (waffle caseiro, casquinha, caneca), leve (com pouca gordura ou não) e orgânico (orgânico ou não). Todos os 15 entrevistados classificaram os dez perfis, fornecendo uma pontuação entre 1 e 10.

Usamos os dados fornecidos pelo [www.xlstat.com](https://bookdown.org/content/1340/www.xlstat.com), descritos em seu [tutorial](https://help.xlstat.com/customer/en/portal/articles/2062346-conjoint-analysis-in-excel-tutorial?b_id=9283) sobre como fazer análises conjuntas no Excel. Faça o download dos dados [aqui.](http://users.telenet.be/samuelfranssens/tutorial_data/icecream.xlsx)




```{r}
library(tidyverse)
library(readxl)


url<-"http://users.telenet.be/samuelfranssens/tutorial_data/icecream.xlsx"
icecream <- tempfile()
download.file(url, icecream, mode="wb")
icecream<-read_excel(path = icecream, sheet = 1)
```


###  Manipulação {.unlisted .unnumbered}


```{r}
icecream[1:10,]

```


Quando inspecionamos os dados, vemos que temos uma coluna para cada entrevistado. Essa é uma maneira incomum de armazenar dados (normalmente temos uma linha por respondente), então vamos reestruturar nosso conjunto de dados com a função de coleta (como fizemos [https://bookdown.org/content/1340/rmanova.html#rmanova](aqui)):

```{r}
icecream <- icecream %>% 
  gather(respondent, rating, starts_with("Individual")) %>% # o entrevistado acompanha o respondente, a classificacao armazena as classificacoes do entrevistado e queremos empilhar todas as variaveis que comecam com Individual
  rename("profile" = "Observations") %>% # renomeia Observations para profile
  mutate(profile = factor(profile), respondent = factor(respondent),  # fatorar identificadores
         Flavor = factor(Flavor), Packaging = factor(Packaging), Light = factor(Light), Organic = factor(Organic)) # fatorar os atributos do sorvete

# Amplo conjunto de dados: uma linha por unidade de observacao (aqui: perfil) e varias colunas para as diferentes observacoes (aqui: respondentes)
# Conjunto de dados longo: uma linha por observacao (aqui: combinacao de perfil x respondente)

# Converter de grande para longo significa que estamos empilhando varias colunas umas sobre as outras. Para isso, precisamos de uma variavel extra para acompanhar qual coluna estamos lidando.
# A funcao de coleta converte conjuntos de dados de amplos para longos.
# O primeiro argumento (respondente) nos dira com qual coluna estamos lidando. Essa eh a variavel que armazenara os nomes das colunas que estamos empilhando.
# O segundo argumento (classificacao) armazenara as colunas reais empilhadas umas sobre as outras.
# Os argumentos a seguir (todas as variaveis com nomes que comecam com Individual) sao as colunas que queremos empilhar.

icecream[1:10,]

```


É melhor usar o Visualizador aqui (clique duas vezes no icecream} objeto no painel Ambiente ou digite View(icecream))} para ver que existem dez linhas (10 perfis) por respondente agora.

As demais variáveis são:

- profile é um identificador para os diferentes sorvetes
- Flavor, Packaging, Light, Organic são os quatro atributos que compõem o perfil de um sorvete

###  Recapitulando: importação e manipulação {.unlisted .unnumbered}

Aqui está o que fizemos até agora, em uma sequência ordenada de operações canalizadas (faça o download dos dados [http://users.telenet.be/samuelfranssens/tutorial_data/icecream.xlsx](aqui):)

```{r}
#library(tidyverse)
#library(readxl)

#icecream <- read_excel("icecream.xlsx") %>% 
 # gather(respondent, rating, starts_with("Individual")) %>% # o entrevistado acompanha o respondente, a classificacao armazena as classificacoes do entrevistado e queremos empilhar todas as variaveis que comecam com Individual
#  rename("profile" = "Observations") %>% 
#  mutate(profile = factor(profile), respondent = factor(respondent),
 #        Flavor = factor(Flavor), Packaging = factor(Packaging), Light = factor(Light), Organic = factor(Organic))
```


## Design de experimentos {.unlisted .unnumbered}


Quando inspecionamos nosso conjunto de dados, vemos que o flavor possui 5 níveis (framboesa, chocolate, morango, manga, baunilha), packaging possui 3 níveis (waffle caseiro, cone, cerveja), a Light possui 2 níveis (baixo teor de gordura versus não), e Organic} tem 2 níveis (orgânico vs. não). O objetivo de uma análise conjunta é estimar até que ponto cada nível de atributo afeta a classificação do sorvete.

Para fazer isso, o fabricante de sorvete poderia criar 5×3×2x2 = 60 sorvetes diferentes e peça às pessoas para avaliarem tudo isso. Isso fornecerá ao fabricante uma boa estimativa da importância de cada atributo e de todas as possíveis interações. No entanto, classificar 60 sorvetes é difícil para os participantes e um estudo tão grande seria caro para o fabricante financiar. Na prática, os pesquisadores nessa situação solicitarão que as pessoas classifiquem um subconjunto desses 60 sorvetes. Nesta seção, discutiremos como selecionar um subconjunto (por exemplo, 10 sorvetes) de todas as combinações possíveis de nível de atributo (ou seja, 60 sorvetes) que ainda nos permitirão obter boas estimativas dos efeitos mais importantes.

No conjunto de dados, já temos as classificações para dez perfis, portanto a decisão de quais sorvetes para teste já foi tomada. No entanto, vamos desconsiderar o fato de já termos os dados e considerar as decisões que precisam ser tomadas antes da coleta de dados. Em outras palavras, vamos discutir como passamos de um fatorial completo (todas as 60 combinações) para um design fracionário (menos de 60 combinações).

A função doe (projeto de experimentos) do pacote radiant} nos ajudará a decidir sobre os projetos de estudo. Radiant é um [pacote do R para business analytics.](https://radiant-rstats.github.io/docs/index.html)

A discussão a seguir da função doe é baseada na discussão da Radiant sobre essa função.

```{r}
#install.packages("radiant")
library(radiant)
```

Para usar a doe, precisamos inserir as informações sobre nossos atributos e seus níveis de uma maneira específica:

```{r}
# attribute1, attribute2, etc. sao vetores com um elemento no qual fornecemos primeiro o nome do atributo seguido por um ponto e virgula e depois fornecemos todos os niveis dos atributos separados por ponto e virgula
attribute1 <- "Flavor; Raspberry; Chocolate; Strawberry; Mango; Vanilla"
attribute2 <- "Package; Homemade waffle; Cone; Pint"
attribute3 <- "Light; Low fat; No low fat"
attribute4 <- "Organic; Organic; Not organic"

# agora combine esses diferentes atributos em um vetor com c()
attributes <- c(attribute1, attribute2, attribute3, attribute4)
```

Agora podemos pedir possíveis projetos experimentais:

```{r}
summary(doe(attributes, seed = 123)) # Seed: fixa o gerador de numeros aleatorios

```

Observe a saída no cabeçalho Design efficiency. Mostra 52 linhas. As linhas representam projetos experimentais com diferentes números de Trials ou diferentes números de sorvetes (ou seja, combinações de nível de atributo) que seriam testados. Uma palavra melhor para julgamento é perfil. Para cada projeto experimental, ele mostra a D-efficiency do projeto - uma medida de como poderemos estimar com clareza os efeitos do interesse após a execução do experimento (pontuações mais altas são melhores) - e se o projeto está ou não equilibrado - se cada nível está incluído no mesmo número de tentativas ou perfis. Idealmente, procuramos projetos balanceados com alta D-efficiency (acima de 0,80 é considerado razoável). Temos dois candidatos, um delineamento experimental com 60 perfis, que é apenas o delineamento fatorial completo ou um delineamento com 30 perfis. Vamos dar uma olhada no design com 30 perfis:

```{r}
summary(doe(attributes, seed = 123, trials = 30))

```


Em Projeto fatorial parcial (ou projeto fatorial fracionário), encontramos os perfis que poderíamos executar em um experimento com 30 em vez de 60 perfis. Sob correlações parciais do projeto fatorial, vemos que dois atributos estão correlacionados, a saber, Light e Organic ($r = -0,105$). Esse sempre será o caso em projetos fatoriais fracionários. Isso significa que algumas combinações de níveis de atributo serão mais prevalentes que outras. Somente em um planejamento fatorial completo todos os atributos serão não correlacionados ou ortogonais.

Um design possível com apenas 10 perfis seria desequilibrado e teria a seguinte aparência:

```{r}
summary(doe(attributes, seed = 123, trials = 10))

```


Comparado ao design com 30 perfis, agora existem mais e mais fortes correlações entre os atributos.

Observe que os perfis não são exatamente iguais aos da experiência usada para coletar os dados do sorvete. Isso ocorre porque, para projetos desequilibrados, existe alguma aleatoriedade na definição das combinações reais de nível de atributo. É também por isso que definimos seed = 123. seed é usado para consertar o gerador de números aleatórios do R. Configurá-lo para um número fixo (123 ou 456 ou qualquer outra coisa) garantirá que o R gere sempre a mesma saída. Sem definir a seed, doe com trials = 10 não daria o mesmo design fracionário toda vez que você a executar.

Observe também que o pacote radiant} instala um suplemento que você pode acessar por meio de suplementos (encontre este botão à direita da linha abaixo de "Arquivo" etc.) -> Iniciar radiant (navegador). Isso abrirá um aplicativo no seu navegador que permitirá executar as etapas acima em uma interface visual intuitiva. Para obter ajuda, confira a discussão da Radiant} sobre o módulo Design de experiências [aqui](https://radiant-rstats.github.io/docs/design/doe.html)


## Um respondente {.unlisted .unnumbered}
###  Estimar valores de peça e pesos de importância {.unlisted .unnumbered}


Embora alguns softwares exijam que você primeiro crie variáveis fictícias representando os níveis de atributo e execute uma regressão, o Radiant} não exige que você faça isso. Você pode simplesmente usar os atributos (cada um com vários níveis) como variáveis. Primeiro, faremos uma análise conjunta dos dados de um respondente (indivíduo 1):

```{r}
respondent1 <- icecream %>% filter(respondent == "Individual 1")

# salve a analise conjunta em um objeto, porque a usaremos como entrada para summary(), plot(), and predict()

conjoint_respondent1 <- conjoint(respondent1, rvar = "rating", evar = c("Flavor","Packaging","Light","Organic")) 

summary(conjoint_respondent1)

```


A saída fornece valores de peça, pesos de importância e coeficientes de regressão. Os valores das partes e os coeficientes de regressão fornecem as mesmas informações: comparado ao nível de referência (o primeiro nível de um atributo; você verá que os valores das partes são sempre zero para este nível), quanto aumenta cada nível de atributo ou diminuir a classificação de um sorvete? Podemos traçar estes resultados:

```{r}
plot(conjoint_respondent1)
```


E então vemos facilmente que essa pessoa desfrutaria de um sorvete com baixo teor de gordura, orgânico, manga ou morango em um cone ou em um waffle caseiro.

Observe que os resultados da regressão conjunta são simplesmente os resultados de uma [regressão linear múltipla:](https://bookdown.org/content/1340/linear-regression.html#regression_without)

```{r}
# Execute essa regressao se estiver interessado em aprender qual preditor eh significativo ou qual eh o R quadrado do modelo geral.

summary(lm(rating ~ Flavor + Packaging + Light + Organic, data = respondent1))

```

Finalmente, os pesos de importância nos dizem com que intensidade cada atributo determina a classificação de um sorvete. Para esse respondente, sabor é o atributo mais importante e luz é o atributo menos importante. A classificação deste respondente é determinada em 59,6% por sabor e em 9,2% por luz.


### Profiles: utilitários previstos {.unlisted .unnumbered}


Prever as classificações (utilitários) dos diferentes sorvetes é muito fácil em R. Primeiro, verifique se temos um conjunto de dados com os diferentes perfis que foram testados:

```{r}
profiles <- icecream %>% 
  filter(respondent == "Individual 1") %>% 
  select(Flavor,Packaging,Light,Organic)

profiles

```

Em seguida, pedimos à função predict para prever as classificações dos perfis com base na função de regressão:

```{r}
predict(conjoint_respondent1, profiles) # prever as classificacoes para os perfis com base na analise conjunta
```

A classificação prevista é mais alta para sorvetes orgânicos com pouca gordura e manga em um waffle caseiro. Mas essas são previsões para sorvetes que o entrevistado realmente classificou. Se quiséssemos saber qual sorvete o entrevistado mais gostava, poderíamos apenas olhar para as classificações observadas (em vez das previstas). É mais interessante obter previsões para sorvetes que o entrevistado não avaliou. Para isso, precisamos dos perfis para todos os sorvetes possíveis. Podemos criar esses perfis com a função expand.grid}. A função expand.grid} usa dois ou mais vetores e cria todas as combinações possíveis de elementos desses vetores:

```{r}
Flavor <- c("Raspberry","Chocolate","Mango","Strawberry","Vanilla")
Organic <- c("Organic","Not organic")

expand.grid(Flavor, Organic)

```

Vamos fazer isso para todos os nossos níveis de atributo:

```{r}
# existe uma maneira mais facil de obter niveis de atributo do que criar os vetores manualmente:

levels(icecream$Flavor) # certifique-se de que o sabor seja fatorado primeiro!

## [1] "Chocolate"  "Mango"      "Raspberry"  "Strawberry" "Vanilla"
# agora crie todos os profiles

profiles.all <- expand.grid(levels(icecream$Flavor),levels(icecream$Packaging),levels(icecream$Light),levels(icecream$Organic)) %>% 
  rename("Flavor" = "Var1", "Packaging" = "Var2", "Light" = "Var3", "Organic" = "Var4") # rename the variables created by expand.grid (don't forget this, otherwise predict won't know where to look for each attribute)

# prever as classificações de todos os perfis

predict(conjoint_respondent1, profiles.all) %>% 
  arrange(desc(Prediction)) # mostrar os sorvetes com a classificação mais alta prevista no topo

```

Mesma conclusão que a da seção anterior: essa pessoa desfrutaria de um sorvete com baixo teor de gordura, orgânico, manga ou morango em um cone ou um waffle caseiro.

## Muitos respondentes {.unlisted .unnumbered}

###  Estimar valores de peça e pesos de importância {.unlisted .unnumbered}

Agora, vamos realizar a análise conjunta no conjunto de dados completo para ter uma idéia de quais sorvetes os 15 entrevistados, em média, gostaram mais e qual a importância de cada atributo:

```{r}
conjoint_allrespondents <- conjoint(icecream, rvar = "rating", evar = c("Flavor","Packaging","Light","Organic")) # como antes, mas com um conjunto de dados diferente.

summary(conjoint_allrespondents) 

```

O sabor é de longe o atributo mais importante. Vamos traçar estes resultados:

```{r}
plot(conjoint_allrespondents)
```

A partir disso, prevemos que, em média, as pessoas mais gostariam de um sorvete de manga orgânico, sem pouca gordura, em um cone.

Os pesos de importância nos dizem com que intensidade cada atributo determina a classificação média de um sorvete. O sabor é o atributo mais importante e a embalagem é o atributo menos importante. A classificação deste respondente é determinada para 59,7% por sabor e para 9,6% por embalagem.

###  Profiles: utilitários previstos {.unlisted .unnumbered}

Vamos prever as classificações de todos os sorvetes possíveis:

```{r}
predict(conjoint_allrespondents, profiles.all) %>% # verifique as secoes anteriores para profiles.all

  arrange(desc(Prediction)) # mostrar os sorvetes com a classificacao mais alta prevista no topo
 
```


Mesmas conclusões de antes: prevemos que, em média, as pessoas mais gostariam de um sorvete de manga orgânico, sem pouca gordura, em um cone.


## Simulação de Mercado {.unlisted .unnumbered}


Digamos que criamos um pequeno número de sorvetes e queremos estimar a participação de mercado de cada um desses sorvetes. Digamos que selecionamos os quatro perfis a seguir:

```{r}
# use slice() para selecionar as linhas 

market_profiles <- profiles.all %>% 
  slice(c(4, 16, 23, 38)) # de profiles.all, selecione as linhas 4, 16, 23, 38 como quatro profiles

market_profiles

#Já sabemos como estimar qual sorvete será mais apreciado:

conjoint_allrespondents <- conjoint(icecream, rvar = "rating", evar = c("Flavor","Packaging","Light","Organic"))

predict(conjoint_allrespondents, market_profiles) %>%
  arrange(desc(Prediction))
 
```

O sorvete de morango com baixo teor de gordura e não orgânico em um cone tem a classificação mais alta prevista entre todos os entrevistados. Mas isso não nos diz qual será a participação de mercado de cada um dos quatro perfis. Para isso, precisamos saber, para cada participante, qual perfil ele escolheria. Em outras palavras, precisamos prever as classificações para cada indivíduo separadamente:

```{r}
# mesmo modelo de antes, mas agora adicione por = "respondent"
conjoint_perrespondent <- conjoint(icecream, rvar = "rating", evar = c("Flavor","Packaging","Light","Organic"), by = "respondent")

predict(conjoint_perrespondent, market_profiles) %>% 
  arrange(respondent, desc(Prediction)) # classificar por respondente e depois por classificacao prevista

```

Vamos reter para cada indivíduo apenas seu perfil mais bem classificado. Podemos fazer isso agrupando por entrevistado e adicionando uma variável denominada ranking que nos dirá o ranking de perfis, com base na classificação prevista, para cada entrevistado:

```{r}
highest_rated <- predict(conjoint_perrespondent, market_profiles) %>% 
  group_by(respondent) %>% 
  mutate(ranking = rank(Prediction))

# dando uma olhada
highest_rated %>% 
  arrange(respondent, ranking)
  

# precisamos reter apenas o sorvete mais bem classificado

highest_rated <- highest_rated %>% 
  arrange(respondent, ranking) %>% 
  filter(ranking == 4)

highest_rated

```



Agora podemos estimar a participação de mercado:

```{r}
market_share <- highest_rated %>% 
  group_by(Flavor, Packaging, Light, Organic) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count))

market_share

```


Vimos que o sorvete de morango, cone, baixo teor de gordura e não orgânico é preferido por 7 em cada 15 participantes, o framboesa, waffle caseiro, sem baixo teor de gordura e sorvete não orgânico é favorecido por 4 em cada 15 participantes e assim por diante .

<!--chapter:end:08-analiseconjunta.Rmd-->

